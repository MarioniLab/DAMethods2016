\documentclass{article}
\usepackage{graphicx}
\usepackage[margin=2.5cm]{geometry}
\usepackage[labelfont=bf]{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{xr}
\usepackage{amsmath}
\usepackage[euler]{textgreek}

\renewcommand{\textfraction}{1.0}
\renewcommand{\floatpagefraction}{.9}
\newcommand\revised[1]{\textcolor{red}{#1}}
\renewcommand{\topfraction}{0.9}    % max fraction of floats at top
\renewcommand{\bottomfraction}{0.8} % max fraction of floats at bottom
\renewcommand{\textfraction}{0.07}  % allow minimal text w. figs

\makeatletter 
\renewcommand{\fnum@figure}{Supplementary \figurename~\thefigure}
\renewcommand{\fnum@table}{Supplementary \tablename~\thetable}
\renewcommand*{\thesection}{Note~\arabic{section}:}
\renewcommand*{\thesubsection}{\arabic{section}.\arabic{subsection}}
\makeatother

%\renewcommand{\thefigure}{S\@arabic\c@figure} 
%\renewcommand{\thetable}{S\@arabic\c@table} 

\externaldocument{short}

\usepackage{url}
\urlstyle{same}

\begin{document}

\begin{titlepage}
\vspace*{3cm}
\begin{center}

{\LARGE
Testing for differential abundance in mass cytometry data
\par}

\vspace{0.75cm}

{\Large 
    \textsc{Supplementary Materials}
\par
}
\vspace{0.75cm}

\large
by

\vspace{0.75cm}
Aaron T. L. Lun$^{1}$, Arianne C. Richard$^{1,2}$ and John C. Marioni$^{1,3,4}$

\vspace{1cm}
\begin{minipage}{0.9\textwidth}
\begin{flushleft} 
$^1$Cancer Research UK Cambridge Institute, University of Cambridge, Li Ka Shing Centre, Robinson Way, Cambridge CB2 0RE, United Kingdom \\[6pt]
$^2$Cambridge Institute for Medical Research,  University of Cambridge, Wellcome Trust/MRC Building, Hills Road, Cambridge CB2 0XY, United Kingdom \\[6pt]
$^3$EMBL European Bioinformatics Institute, Wellcome Genome Campus, Hinxton, Cambridge CB10 1SD, United Kingdom \\[6pt]
$^4$Wellcome Trust Sanger Institute, Wellcome Genome Campus, Hinxton, Cambridge CB10 1SA, United Kingdom \\[6pt]
\end{flushleft}
\end{minipage}

\vspace{1.5cm}
{\large \today{}}

\vspace*{\fill}
\end{center}
\end{titlepage}

\begin{color}{red}
\section{Defining the hypersphere parameters}

\subsection{General comments}
We assume that appropriate transformations (e.g., biexponential \cite{parks2006new}) have been applied to the marker intensities for each cell.
We also assume that normalization of marker intensities between samples, if necessary, has already been performed (see Section~\ref{sec:intnorm}).
In data sets where samples were barcoded and multiplexed for staining and processing \cite{zunder2015palladium}, normalization of intensities is not required as the same technical biases should be present in all samples and thus should cancel out when comparing between samples.
Assignment of cells to hyperspheres is then performed using the transformed (and normalized) intensities for each cell.

At this point, it is important to stress that, while each hypersphere contains a subset of cells in the population, this should not be confused with a cell subpopulation.
The latter refers to a biologically meaningful or functional subset of cells, while the former is simply an analytical construct used for cell counting.

\subsection{Centring on existing cells}
Each hypersphere is centred at a point defined by an existing cell.
This is necessary because there are an infinite number of hyperspheres in the $M$-dimensional space.
Centring on existing cells ensures that only non-empty hyperspheres are considered.
In practice, we further reduce computational work by only constructing hyperspheres at every (randomly chosen) 10\textsuperscript{th} cell.
This avoids redundant work in high-density subspaces where many of the resulting hyperspheres would be near-identical in terms of their positions and cell counts.
Note that many adjacent hyperspheres will be overlapping, which has some implications for FDR control.

\subsection{Choosing the hypersphere radius}

\subsubsection{Overview}
The radius $r$ of each hypersphere dictates the trade-off between count size (the number of cells assigned to a hypersphere) and spatial resolution (the ability to distinguish between adjacent subspaces).
Our choice of radius $r=0.5\sqrt{M}$ increases with the number of markers $M$, to avoid problems with sparsity at high dimensions; and it acounts for the variability of the intensities for each marker, whereby cells with a 10-fold difference in marker expression are routinely assigned to the same hypersphere (this is motivated by noting that technical or biological variability often results in an order of magnitude difference in observed expression within the same functional subpopulation \cite{ornatsky2008study,zunder2015continuous,zunder2015palladium}).
This is described in more detail in the following text.
\end{color}

\subsubsection{Formulating an expression for the radius}
Consider a subpopulation where the intensities of each marker are distributed around some mean intensity with standard deviation $r_0$.
The expectation of the squared Euclidean distance between any two cells in this subpopulation is $2r_0^2M$.
Now, construct hyperspheres centred at each of the cells in the subpopulation.
Setting the hypersphere radius as $r=r_0\sqrt{2M}$ ensures that most hyperspheres will contain a large proportion of cells from the same subpopulation.
Indeed, a simple simulation with Normally-distributed intensities for 30 markers indicates that the majority of hyperspheres contain more than 50\% of the subpopulation.

% To wit, E[(Xa - Xb)^2] = E(Xa^2) + E(Xb^2) - 2E(XaXb)
%                        = r0^2 + u^2 + r0^2 + u^2 - 2u^2
%                        = 2r0^2
% And then sum this over all markers. We can demonstrate with some code:
%   a <- matrix(runif(30000, 0, r), ncol=30)
%   out <- as.matrix(dist(a))
%   sqrt(var(a[,1])) * sqrt(30) * sqrt(2)
%   mean(out)
% The same works with normally distributed intensities:
%   r <- 1
%   a <- matrix(rnorm(30000, 1, r), ncol=30)
%   out <- as.matrix(dist(a))
%   sum(colSums(out <= r*sqrt(2*ncol(a))) >= nrow(a)/2)

We further assume that the expression of a marker will differ by up to 10-fold across functionally similar cells, due to technical noise or biological variability within the same subpopulation.
(Expression profiles for more variable markers like CD25 may consist of multiple internal subpopulations.)
For typical analyses involving transformed intensities on the $\log_{10}$-scale, an interval of length 1 will span an order of magnitude in marker expression.
When centred around the mean, this interval should contain most of the intensity distribution for a single subpopulation.
According to Chebyshev's inequality, the interval defined by a distance of $r_0\sqrt{2}$ from the mean will contain at least 50\% of the values from an arbitrary distribution.
We equate these interval lengths to obtain a rough estimate for the standard deviation of the intensities, i.e., $2r_0\sqrt{2} = 1$.
Applying this to the radius yields $r=0.5\sqrt{M}$, meaning that each hypersphere will be able to include cells with 10-fold differences in expression for one or more markers.
Thus, each hypersphere will contain enough counts for further analysis, even when the intensities vary across an order of magnitude.

% This always does my head in a bit; Chebyshev is conservative in that the root-2 is a larger than it should be for most distributions.
% However, this means that the estimate of r_0 is lower than what it should be for most distributions.
% All this does is provide us with a bit more precision at the cost of fewer counts, so I guess that's fine.

% (A lower proportion of cells in the subpopulation will be counted if the hypersphere is not centred at the subpopulation centre.
% This will result in reduced power to detect differential abundance for that particular hypersphere.
% However, as long as there is one hypersphere that is centred at the subpopulation centre, then the potential to reject the null hypthesis for the corresponding subpopulation will not be lost.
% Recall that each hypersphere is centred at an observed cell, so any subpopulation in the data set with a reasonable number of cells ($>100$) is likely to be captured by an appropriate hypersphere near the subpopulation centre.)

We verify the choice of formulation of $r$ by examining the distance between each cell $x$ and its neighbours in the first sample from each time course.
The nearest neighbours of $x$ are identified in the full $M$-dimensional space and represent other cells in the same subspace as $x$.
As the number of markers increases, the distance to the neighbours increases at a rate that is roughly consistent with the square root function (Supplementary Figure~\ref{fig:radius}).
This justifies the use of the relation $r\propto\sqrt{M}$.
With increasing $M$, the radius will also increase such that a hypersphere centred at $x$ will still contain the neighbours of $x$, i.e., sensitivity of counting for cells in the same subspace is preserved.
In contrast, cells in other subspaces will cease to be assigned to the hypersphere as they become separated from $x$ on other markers, i.e., specificity of counting is improved.
 
% A consequence of setting $r$ like this is that, in any one dimension, it is possible to count cells that are more than $r_0$ from the centre of the hypersphere.
% This is inevitable in multi-dimensional space, where you need to account for scatter in dimensions other than the one of immediate interest.
% Setting $r$ any smaller would fail to count the majority of cells derived from the same subpopulation.
% Indeed, a more straightforward definition of each subspace would be to only count cells that are less than $r_0$ away from the centre in every dimension.
% This defines the subspace as an $M$-dimensional box with sides of length $2r_0$.
% However, the use of boxes is not practical as the probability of cells falling into a box will decrease with increasing $M$. 
% This results in near-zero counts (or counts of 1, for boxes centred on an existing cell).
% Hyperspheres are more suitable at large $M$, which motivates their use above.
%
% Also note that the use of the word 'specificity' assumes that the nearest neighbors identified in the full space as the "true" nearest neighbors.
% This won't be the case if not all markers are informative, such that increasing the number of dimensions will just increase noise.
% In such cases, specificity may actually decrease due to the curse of dimensionality, whereby it is harder to distinguish neighbors properly.
% This motivates the careful selection of markers for hypersphere formation.
% It also ties into situations where you don't necesssarily care about how certain markers interact with each other.
% For example, in signalling, you might only be interested in how each signalling molecule behaves by itself.
% In such cases, there's no need to consider the high-dimensional space involving all signalling molecules.
  
For the Oct4-GFP and Nanog-Neo time courses, the radius is consistent with the median distances from each cell to its 10\textsuperscript{th} nearest neighbour.
This means that around 10 cells from the subspace will be assigned to each hypersphere when $r_0\sqrt{2}=0.5$.
Combined with counts from the other samples in the time course, this provides enough information to reliably detect DA hyperspheres.
In contrast, the radius in the Nanog-GFP time course is consistent with the distance of each cell to its 1\textsuperscript{st} nearest neighbour.
This is because the first sample in the Nanog-GFP time course contains fewer than 3000 cells, while samples in the other time courses contain over 10000 cells.
The sparsity of cells in this sample results in fewer neighbours at any given distance.
For experiments with few cells across all samples, a larger $r_0$ may be required to obtain sufficient counts for further analysis.
(Note that we keep $r_0\sqrt{2}=0.5$ for Nanog-GFP, as the other samples in the time course contain 5-10-fold more cells.
Thus, counts will be large enough to detect differences later on.)

\subsubsection{Additional remarks on radius choice}
\label{sec:additionalradius}

The suitability of the default setting of $r_0\sqrt{2}=0.5$ can be assessed for each data set by examining the distance from each cell to its nearest neighbours.
Supplementary Figure~\ref{fig:nvd} shows these distances for each cell in the Oct4-GFP reprogramming time course.
Here, the default radius corresponds to the median of the distances from each cell to its 15\textsuperscript{th} nearest neighbour.
This means that half of all hyperspheres will contain more than 15 cells, which is large enough for downstream hypothesis testing.
In contrast, a smaller value of the radius (e.g., $r_0\sqrt{2}=0.4$) would not even include the nearest neighbour for each cell on average.
This would lead to a count of 1 for most hyperspheres, which is not sufficient for further statistical analysis.

% Yes, a count of 10-15 is enough if the changes are extreme.
% Even if the average is low, you should be able to reject the null under a NB model.
% This differs from the motivation of the later filtering steps, which focuses on changes that are of interest rather than those that are possible.

A consequence of working in high-dimensional space is that small changes to the radius will result in large changes to the counts.
For example, in an experiment consisting of 30 markers, increasing the radius by 10\% will increase the hypersphere volume (and the potential number of counted cells) by over 17-fold. 
In this respect, the analysis is quite sensitive to the choice of radius.
However, a more relevant assessment of sensitivity is to consider changes in the counts per hypersphere.
We repeated the DA analyses using radius values that approximately halved or doubled the median number of counted cells in each hypersphere relative to the default (Supplementary Figure~\ref{fig:nvd}).
We observed that most of the DA hyperspheres from the default analysis were still detected in the repeated analyses (Supplementary Table~\ref{tab:param}).
Over 60\% of the DA hyperspheres were recovered with a smaller radius, while using a larger radius detected almost all of them (and an extra 35\%).
These differences are expected because larger hyperspheres have greater counts and more evidence to reject the null hypothesis, albeit at the cost of spatial resolution.
Nonetheless, the general similarity in the results indicates that the analysis is robust to substantial changes in the size of the counts.

An obvious question is whether an ``optimal'' value of $r_0$ can be obtained for any given data set.
This value depends on the variability of marker intensities in each subpopulation.
For example, setting $r_0$ to the standard deviation of the marker intensities \textit{for a single subpopulation} allows more cells associated with that subpopulation to be counted into the hypersphere, while reducing the chance of counting cells from adjacent subpopulations.
However, the standard deviation of each subpopulation is not easy to estimate empirically, as we do not know the true number of subpopulations in the $M$-dimensional space.
Indeed, a single optimal value may not exist, e.g., if different subpopulations or markers have different standard deviations.

A suboptimal choice of $r_0$ is not a critical concern for a DA analysis.
A value of $r_0$ that is too small will reduce power to detect a DA subspace as the counts are too low.
A value that is too large will also reduce power through the loss of spatial resolution -- specifically, cells from non-differential subspaces will be included in the counts for a hypersphere in a differential subspace, reducing the log-fold change in abundance.
In both cases, power is affected rather than error rate control.
While loss of power is undesirable, the analysis will still be valid as any discoveries (that are made in spite of the diminished power) can be trusted.
We note that ``contamination'' of a non-DA hypersphere with cells from differential subspaces is also possible for large $r_0$, which could lead to the detection of the former as a false positive.
However, the effect of this contamination is mitigated by the fact that the position of each hypersphere is defined using the median intensities.
Any contamination that substantively changes the counts for a hypersphere will also shift its position towards the differential subspace.
Thus, a contaminated hypersphere that is erroneously classified as DA will be assigned to or near the differential subspace (based on its position), rather than a non-differential subspace (based on its centre).
This reduces the detection rate of false positives in the non-differential subspace.

To demonstrate this effect, we set up a simulation where we examined the effect of increasing the radius on the position of each hypersphere (Supplementary Figure~\ref{fig:radius_schematic}).
When larger radii were used, hyperspheres centred on cells from a non-DA subpopulation exhibited larger log-fold changes in abundance (Supplementary Figure~\ref{fig:radius_position}).
This is consistent with increasing contamination from cells in the neighbouring DA subpopulation.
However, the positions of the affected hyperspheres were also shifted towards the DA subpopulation.
In most simulation scenarios, this shift was sufficiently large that the hyperspheres with large log-fold changes ($>1$) were indistinguishable from hyperspheres centred on cells from the DA subpopulation itself.
The only exception occurred when the size of the DA subpopulation was reduced, such that some of the non-DA hyperspheres had large log-fold changes but lay outside the subpopulation boundaries.
Even in this case, a substantial shift in position towards the DA subpopulation was observed, mitigating the effect of misinterpreting these hyperspheres as part of a differential subspace.
Using very large hypersphere radii also results in a decrease in the log-fold changes for hyperspheres centred on cells from the DA subpopulation.
This is consistent with loss of power when cells from non-DA subspaces are included in the counts.

\subsection{Using weighted medians to compute the hypersphere position}
\revised{The median position is more appropriate than the hypersphere centre for characterising the location of the hypersphere, when the cells assigned to the hypersphere are not symmetrically distributed around the centre.}
When calculating the median-based position for a hypersphere $h$, each cell from sample $s$ is assigned a weight of $T_s^{-1}$ where $T_s$ is the total number of cells in $s$.
For each marker, the weighted median of intensities from all cells in $h$ is computed.
The set of weighted medians for all markers represents the coordinates of the position of $h$.
This weighting scheme ensures that the calculation of the position is not dominated by large samples with many cells.
Obviously, it converges to a simple median in data sets containing samples of similar size.

\subsection{Choosing a transformation for the intensities}
The interpretation of the default hypersphere radius assumes that the transformed intensities lie at or close to a $\log_{10}$ scale.
We use the logicle (or biexponential) transformation \cite{parks2006new} implemented in the flowCore package \cite{hahne2009flowcore}, which is linear around zero but approaches the $\log_{10}$ scale when applied to high values for the raw intensities.
Other transformations can be used provided that this scale is approximately maintained.
For example, the inverse hyperbolic sine function converges to the logarithmic function at high values.

In general, we do not scale the intensities to equalize the standard deviation of the intensity distribution across markers.
This is because differences in the range of expression between markers may be biologically interesting and should be preserved.
For example, in a population of T cells, CD25 may be expressed across a wide range of intensities reflecting a range of different activation states.
In contrast, CD3 (a classical T cell marker) should be present at high intensities in all cells with low variance across the population.
Scaling to equalize the standard deviations for all markers would compress the CD25 intensity distribution and compromise the resolution of potentially relevant subpopulations, while also amplifying irrelevant changes in CD3 expression.
Without scaling, more highly variable markers will dominate the placement of hyperspheres.
This is desirable as it ensures that biological differences between subpopulations can be captured. 

\section{Strategies for dealing with intensity shifts}

\subsection{Overview}
In barcoded experiments, technical effects causing shifts in marker intensity between samples are avoided \cite{zunder2015palladium}.
This is due to use of multiplexed staining and cytometry, which ensures that any fluctuations in the experimental procedures (e.g., staining efficiency, cellular concentration, detector sensitivity) affect all samples equally.
While barcoding is becoming more common \cite{gaudilliere2014delayed,gaudilliere2015implementing}, its application is not always possible.
For example, if the number of samples is greater than the number of available barcodes, samples will need to be split into batches for separate barcoding and analysis.
This may introduce intensity shifts between batches, where similar cells in different batches have different intensities due to technical factors.
For hypersphere counting, these shifts are problematic as cells from the same subpopulation may no longer be counted into the same hypersphere across samples.
This has some detrimental consequences for the downstream statistical analysis (see below).
Here, we present some strategies for handling intensity shifts in several scenarios.

\subsection{Intensity shifts between separately barcoded batches}
\label{sec:intnorm}

\subsubsection{Outline of the normalization method}
Consider an experimental design containing multiple batches of separately barcoded samples, such that intensity shifts are present between but not within batches.
Intensities can be normalized between batches by assuming that the pooled intensity distribution across samples in each batch should be similar between batches (i.e., if intensity shifting were not present).
This is usually reasonable in experimental designs where each batch contains samples from the same or similar set of conditions.
The aim of the normalization procedure is to transform the pooled intensity distributions for all batches towards some reference distribution, thus removing any intensity shifts or changes in the shape of the intensity distribution between batches.

\subsubsection{Creating a weighted distribution of intensities}
The first step is to create a weighted distribution of intensities for each batch.
\begin{enumerate}
    \item Let the number of samples for condition $c$ in batch $b$ be denoted by $S_{cb}$.
        The average sample size $\bar{S}_c$ for condition $c$ is computed by taking the mean of $S_{cb}$ across all batches.
    \item Denote the number of cells in sample $s$ of condition $c$ in batch $b$ as $T_{scb}$.
        Each cell in $s$ has a weight of 
        \[
            w_{scb} = \frac{\bar{S}_c}{S_{cb}T_{scb}} \;.
        \]
    \item Cells are pooled across all samples within each batch.
        A weighted distribution of intensities is generated for each marker $m$, based on the intensities and weights for all cells in the batch.
        This is used to generate a batch- and marker-specific quantile function $q_{mb}(.)$ for the intensity distribution.
\end{enumerate}
The weights ensure that the contribution of each condition is the same across batches, to accommodate experiments where $S_{cb}$ differs between batches.
They also ensure that the contribution of each sample is the same within each batch, if $T_{scb}$ varies between samples.
Otherwise, the pooled intensity distributions may not be similar across batches if some samples or conditions contribute more to one batch than to others.
Note that construction of $q_{mb}(.)$ is only performed using samples from conditions that are present in \textit{all} batches.
If a batch is missing a condition, no amount of weighting can overcome this complete lack of information.

\subsubsection{Performing range normalization across batches}
A simple approach is to scale the marker intensities so that the range of the weighted distribution is the same for each batch.
We compute the minimum and maximum for each batch $b$ as $q_{mb}(0.01)$ and $q_{mb}(0.99)$, i.e., the 1\textsuperscript{st} and 99\textsuperscript{th} percentiles of the reference distribution for marker $m$, respectively.
(We use percentiles rather than taking the actual extremes to protect against outliers.)
The reference minimum and maximum are defined as the average of $q_{mb}(0.01)$ and $q_{mb}(0.99)$, respectively, across all batches.
For each batch $b$, a linear scaling function is defined that converts $q_{mb}(0.01)$ and $q_{mb}(0.99)$ to the reference minimum and maximum, respectively.
This function is then applied to the intensities of $m$ for all samples in $b$, yielding normalized intensities where differences in the location and spread of the distribution between batches are eliminated.
However, this ``range-based'' normalization assumes that the technical differences between batches can be fully described as a linear transformation of intensities.
More complex non-linear effects are not supported.

\begin{color}{red}
\subsubsection{Performing warping normalization across batches}
An alternative strategy is to exploit existing methods for automatic normalization of flow cytometry data.
We adapt the approach used by the flowStats package (https://www.bioconductor.org/packages/flowStats).
\begin{enumerate}
    \item For each batch $b$, randomly sample $T_b$ cells with replacement from the weighted distribution of intensities,
        where $T_b$ is the total number of cells across all samples in $b$.
        This is necessary as the flowStats implementation does not consider weights, which are instead reflected in the sampling probabilities.
    \item Apply the normalization() function in flowStats to compute a monotonic warping function.
        Briefly, this identifies high-density landmarks (i.e., peaks) in the intensity distribution for each batch.
        Landmarks from all batches are pooled together and clustered based on their locations, i.e., intensity at the peak summit.
        A smooth monotone ``warping'' function \cite{ramsay2002applied} is defined for each batch to align the location of each of its landmarks to that of other landmarks (from different batches) in the same cluster.
    \item Apply the warping function for batch $b$ to the intensities for marker $m$ in all samples from $b$.
        This yields normalized intensity values that are comparable across batches.
\end{enumerate}
This approach is more flexible than range-based normalization as it accommodates non-linear shifts across the intensity distribution.
However, it relies on accurate identification of landmarks that can be matched across batches.
This may not be possible in noisy data sets with strong inter-batch heterogeneity.

\subsubsection{General comments about inter-batch normalization}
Both of the normalization methods above require a semi-balanced design with samples from at least one group in each batch.
Ideally, at least one sample from each group would be present in each batch, such that all samples would be able to contribute to the weighted distribution for each batch.
Though some conditions may not contribute to the construction of the weighted distribution in any batch, the scaling/warping functions can still be applied to the intensities from samples in those conditions.
This allows normalization of intensities for samples in conditions that are not present in all batches.
However, the obvious caveat is that intensities outside the range of values used to construct the functions may not be accurately normalized.

Both methods also assume that the systematic differences between batches are primarily driven by technical factors.
Any changes in the intensities are considered to be uninteresting and normalized out.
However, this may not be appropriate if batches are confounded with differences in the biological conditions.
In the simplest case, an increase in marker intensity might be due to differences in staining efficiency between batches, or a genuine change in protein expression between conditions.
Confounding factors can be avoided with good experimental design, though some subtleties need to be considered -- see the example below.
\end{color}

\subsubsection{Application of the normalization methods to real data}
As a demonstration, we applied our normalization procedure to the BMMC data set generated by Levine \textit{et al.} \cite{levine2015datadriven}.
This data set consists of stimulated and unstimulated samples from each of multiple donors, where barcoding was performed within but not between individuals, i.e., each individual represents a batch.
Before normalization, we observed large differences in the intensity distribution of some markers between corresponding samples from different individuals (Supplementary Figure~\ref{fig:quantile}).
Part or all of these differences are likely caused by the technical effects of separate staining and cytometry between individuals.
Shifts in the maximum intensity were successfully eliminated upon range-based normalization.
Application of quantile normalization also removed changes in the shape of the intensity distribution between individuals.

It is worth noting, however, that this experimental design is not ideal for our normalization methods.
This is because the technical differences between batches are confounded by genuine biological differences between individuals.
Subsequently, normalization to remove the former may also remove or distort the biological effects.
A more suitable design would contain samples from multiple individuals in each batch, such that one could assume that there were no biological differences between the average of individuals within each batch.
To mitigate any distortions of the underlying biology in this data set, we only apply range-based normalization as this is more restrained in how it corrects for the inter-batch differences.
Nonetheless, the DA analysis following our normalization still yields sensible results, as described in Section~\ref{sec:bmmc}.

\subsection{Intensity shifts between non-barcoded samples}
A more difficult situation is that of a data set containing samples with no barcoding at all.
For an intensity shift between two samples in different conditions, it is impossible to determine whether the shift represents a technical or biological effect.
Subsequently, normalizing the location of the intensity distributions risks discarding interesting biology.
Instead, we use a different approach where the radius of the hyperspheres is increased by the size of the shift.
Cells from the same subpopulation in different samples are more likely to be counted into the same expanded hypersphere, even after shifting of intensities between samples.

To calculate the magnitude of the intensity shift due to technical effects, we compute the mean intensity of each marker in each sample.
This yields a vector of means $\mathbf{u}_m$ for each marker $m$.
We fit a linear model to $\mathbf{u}_m$ for each $m$, using an appropriate design matrix that describes the experimental set-up.
The residual variance of the fitted model provides an estimate of the average squared shift between replicate samples.
As this component of the shift occurs between replicates, it is more likely to be technical in origin.
We average the variance estimates across all markers to obtain $v^2$, the extra variance introduced by the shifting process.
Stochastic intensity shifts between samples increase the squared Euclidean distances between cells (from the same subpopulation but in different samples) by $2v^2$ for each marker.
Thus, the new hypersphere radius should be $\sqrt{2M(r_0^2 + v^2)}$ in order to be able to routinely assign such cells to the same hypersphere.

% This is a logical extension of the previous statements. If you consider that cells 'a' and 'b' in different samples get an extra random 'Ya' and 'Yb':
%   E[(Xa + Ya - Xb - Yb)^2] = 2(r0^2 + v^2)
% Or for some code:
%   sqrt(mean((rnorm(10000, sd=0.5/sqrt(2)) + rnorm(10000, sd=0.25)
%            - rnorm(10000, sd=0.5/sqrt(2)) - rnorm(10000, sd=0.25))^2))
%   sqrt(0.25^2 * 2 + 0.5^2)

We tested this approach with simulations based on the reprogramming time courses.
Simulated data were generated in the same manner as described for assessing type I error control, using only the first sampling scheme for simplicity.
Then, for each sample and for each marker, we sampled an intensity shift from a Normal distribution with a mean of zero and a standard deviation ranging from 0 to 0.3.
This shift value was added to the marker intensities for all cells in that sample.
We counted cells into hyperspheres with and without expansion of the radius, performed the DA analysis and computed the observed type I error rate.
For small shifts, the type I error rate was controlled with both the default and expanded radii (Supplementary Figure~\ref{fig:shift_sim}).
This is due to an increase in the negative binomial dispersion, which accounts for greater variability in counts between replicates when cells from the same subpopulation are shifted in or out of the hypersphere between samples. 
For larger shifts, increases in the dispersion were not sufficient to control the type I error rate with the default radius.
This is because the movement of entire subpopulations in or out of hyperspheres yields cell counts that are not accurately modelled with the NB distribution.
In contrast, the use of an expanded radius mitigates the loss of error control.
This reduces the effect of the shifts on the cell counts for each hypersphere, which allows for smaller dispersions and ameliorates the inaccuracy of the NB model.

There are important caveats with this expansion approach, some of which are mentioned below.
Firstly, we assume that the variance of the shifting process is the same for different markers.
This may not be true if some antibodies or detector channels are more susceptible than others to technical effects.
Secondly, we assume that shifts in location between replicates are wholly technical.
This is unlikely to be true, given that biological variability will be present between, e.g., replicate mice or patients.
Thirdly, as observed above, accurate control of the type I error rate is not actually achieved upon expansion.
The improvement is only a mitigation of the more severe loss of control when the default radius is used.
Finally, expansion of the hypersphere radius leads to loss of spatial resolution, which can reduce power to detect changes in abundance.
While these shortcomings are significant, the lack of barcoding in the data itself leaves few analytical options for rigorous quantitation across samples.
In such cases, radius expansion seems to be the best strategy.

\section{A brief description of edgeR's statistical framework}

\subsection{Quasi-likelihood negative binomial generalized linear models}
The following section largely paraphrases Lun \textit{et al.} \cite{lun2016delicious}, with some minor adjustments.
Consider a hypersphere $h$ with count $y_{hs}$ in each sample $s$.
In edgeR, the count in each sample is modelled with a quasi-negative binomial distribution with mean $\mu_{hs}$.
(We will discuss the dispersion parameters of this distribution later.)
Using a log-link generalized linear model with $G$ coefficients \cite{mccarthy2012differential}, the mean is represented as
\[
    \log \mu_{hs} = \sum_{g=1}^G x_{sg} \beta_{hg} + o_{hs} \;,
\]
where $\beta_{hg}$ is the hypersphere-specific value of coefficient $g$, $x_{sg}$ is the sample-specific predictor for $g$, and $o_{hs}$ is the hypersphere- and sample-specific offset.
For simple one-way layouts, each coefficient represents the (log-transformed) average proportion of cells within a group, and each predictor specifies the group to which each sample belongs.
More complex designs can also be used where coefficients represent blocking factors or real-valued covariates.
The offset for each sample is defined as the log-transformed total number of cells, and ensures that differences in the numbers of cells between groups do not cause differences in $\beta_{hg}$.

With the quasi-likelihood methods in edgeR, the mean-variance relationship of each count is modelled as
\[
    \mbox{var}(y_{hs}) = \sigma^2_{h} (\mu_{hs} + \mu_{hs}^2 \phi_h) \;,
\]
where $\sigma^2_h$ is the shrunken quasi-likelihood dispersion and $\phi_h$ is the trended negative binomial dispersion.
Any increase in the observed variance of the counts will be modelled by an increase in these two dispersions.
\revised{(We stress that, in our method, the dispersions model the variability of the cell counts across replicates for each hypersphere, \textit{not} the variability of the marker intensities across cells.)}
The NB dispersion for each hypersphere is set to the fitted value of a mean-dispersion trend \cite{mccarthy2012differential}, i.e., $\phi_h = \phi(\mu_h)$ where $\phi$ is the mean-dependent trend function and $\mu_h$ is the average count across all samples for each hypersphere $h$.
This allows empirical mean-variance relationships to be accurately modelled.
The raw QL dispersion for each hypersphere is estimated from the deviance of the fitted GLM.
A separate mean-dependent trend is fitted to the raw QL dispersions against $\mu_h$ for all hyperspheres, and robust empirical Bayes shrinkage is performed to squeeze the raw estimates towards this trend \cite{lund2012detecting,phipson2016robust}.
The resulting values are referred to as the shrunken QL dispersions $\sigma^2_h$, and allow hypersphere-specific variability to be modelled in the presence of limited replication.

Hypothesis testing is performed by formulating null hypotheses in terms of the various $\beta_{hg}$.
For example, in a one-way layout with groups $g\in\{1,2\}$, one could test for differential abundance between groups by testing the null hypothesis $\beta_{h1}=\beta_{h2}$ for each hypersphere.
A similar approach can be used for more complex designs -- for example, if a spline is fitted to the abundances with respect to time, the corresponding coefficients can be set to zero under the null to test for any time effect.
The $p$-value for each hypersphere is calculated with the QL F-test, which accounts for the uncertainty in estimating the shrunken QL dispersions. 

\subsection{Filtering hyperspheres on their average abundances}
\revised{As an aside, edgeR assumes that the input data have already been filtered to remove hyperspheres with low average counts.
Hyperspheres with very few cells do not have enough evidence to reject the null hypothesis, even if they did contain genuine changes in abundance.
Discarding them reduces the total number of tests and mitigates the severity of the multiple testing correction \cite{bourgon2010independent}.
It also reduces the amount of computational work and avoids problems with discreteness when fitting the mean-dispersion trends in edgeR.
By default, we use an average count threshold of 5 in our analysis, i.e., a hypersphere must contain an average of 5 cells or more across all samples to be retained.
This provides a good compromise between retention of potential DA hyperspheres and removal of the majority of low-abundance hyperspheres,
by focusing on detection of changes in cell subpopulations of moderate-to-high abundance that are likely to constitute the major differences between conditions. 
Lower thresholds can also be used, e.g., to identify rare DA subpopulations, but this may not result in the detection of more hyperspheres. 
While more hyperspheres will be retained, the effect of the multiple testing correction will increase concomitantly, likely reducing power.}

\subsection{Normalizing the hypersphere counts across samples}
\revised{As mentioned above, we define the GLM offsets as the log-transformed total number of cells per sample.
This means that GLM is effectively modelling the \textit{proportion} of cells in each sample that are located inside each hypersphere.
Thus, differences in the input quantities of cells between different samples will not result in spurious DA.
Obviously, this means that global changes in abundance are not detected -- for example, if the abundances of all cell types increase by the same fold-change, it would not affect a change in the proportions.
We disregard such changes as these are inherently confounded with varying input quantities in most real experiments.
We ignore hypersphere-specific biases (i.e., systematic differences in abundance between hyperspheres caused by technical effects like staining efficiency) as these are expected to cancel out when comparing counts from the same hypersphere.
The use of barcoding and multiplexing also avoids introducing sample-specific biases related to differences in staining efficiency or machine behaviour.}

\revised{Note that our strategy for normalizing the count data does not protect against composition effects.}
Composition effects refer indirect changes to the proportion of cells assigned to a subspace, caused by changes to the total number of cells between conditions.
For example, consider a situation where one subspace experiences a large increase in cell abundance between conditions, while the abundances in all other subspaces remain the same.
The increase in the former drives an increase in the total number of cells in the affected condition.
This results in a decrease in the \textit{proportion} of cells that are assigned to the other subspaces, leading to differences being observed in hyperspheres that do not directly exhibit a change in abundance.
From a mathematical perspective, this is not incorrect as the proportions \textit{do} change when the total counts are altered.
Nonetheless, detection of such subspaces may not be biologically relevant, so composition effects should ideally be removed prior to further analysis.
Unfortunately, conventional strategies for normalizing these effects (from RNA-seq data analysis \cite{robinson2010scaling}) are not applicable here.
If these methods were applied to the hypersphere counts, they would assume that most hyperspheres are not differentially abundant.
This is unlikely to be true in many settings, e.g., due to large-scale changes upon stimulation or activation.

Rather, alternative strategies are required to mitigate composition effects.
The simplest approach is to gate out any high-abundance differential subpopulations.
The total count can then be calculated from the remaining cells, which avoids introducing composition effects from the differential subpopulation.
Identification of problematic subpopulations can be done before the DA analysis based on existing knowledge, or afterwards based on the top DA hyperspheres.
For example, consider a mixed population of T and B cells.
    If these cells were analyzed together, a large increase in the number of B cells in one condition would result in a concomitant decrease in the \textit{proportion} of T cells -- even if the number of T cells did not change between conditions.
    This result would suggest that the T cell population decreases in abundance, which might be misleading.
Instead, one can gate on CD3 or CD19 to isolate T or B cells, respectively, and then analyze each of the gated populations separately.
This avoids detecting indirect changes in T-cell subpopulations due to changes in B-cell abundance, and vice versa.
Note that this is only necessary for DA subpopulations with many cells, as changes in small subpopulations will not have a substantial effect on the total count.

Another approach is to test for differential abundance against a minimum log-fold change threshold.
This avoids detecting small changes in abundance caused by composition effects.
Such changes may be statistically significant but are unlikely to be biologically relevant \cite{mccarthy2009treat}, and are subsequently ignored.
While this is a more general approach than gating, it only protects against small composition effects -- large changes in the dominant subpopulation may induce large changes in abundance across all hyperspheres.

\begin{color}{red}
\subsection{Assessing edgeR's performance on mass cytometry count data}
\label{sec:edgeRsim}
We tested the performance of edgeR using simulations constructed from the MEF reprogramming study.
For each MEF time course, we pooled cells from all associated samples.
We generated new samples by randomly sampling cells from the pool.
Each new sample contained the same number of cells as one of the original samples.
We separated the new samples into two groups (i.e., biological conditions), counted cells into hyperspheres and tested for DA between groups using edgeR.
Here, we used a design matrix with a one-way layout to fit a GLM to the counts for each hypersphere.
A contrast was constructed to test for differences between groups, as described above.
As a comparison, we also tested the performance of the Mann-Whitney test, which is often used to detect differential proportions in flow cytometry data \cite{watson1992significance} and has been applied to mass cytometry data for the same purpose \cite{behbehani2015mass}.
Each count in each hypesphere was converted into a proportion of the total number of cells from the corresponding sample.
The Mann-Whitney test was applied to these proportions to test for significant differences between groups, using the wilcox.test function in R.

We used two sampling schemes for this simulation.
The first scheme involved sampling with replacement from the pool with a constant probability of sampling each cell.
This is the simplest approach but assumes that only sampling noise contributes to variability between replicates.
In practice, additional biological variability will be present due to differences in the composition of cell populations extracted from replicate animals or cultures.
To represent this, each cell $j$ in the pool was assigned a probability weight $R_{js}$ for sample $s$.
($R_{js}$ was sampled from a Gamma distribution with shape and rate set to 0.01.
These parameters were chosen to yield NB dispersions of 0.5-1.5 per hypersphere, comparable to values observed in Supplementary Figure~\ref{fig:nbdisp} for real data.
In contrast, the first sampling scheme yields near-zero estimates.)
The probability of sampling cell $j$ in sample $s$ is proportional to $R_{js}$, thus skewing selection towards a particular subset of cells in that sample.
However, as the values of $R_{js}$ differ between samples, the favoured subset will also be different for each replicate.
Thus, the weighting introduces extra variability by changing the cell composition between replicates.
Note that the null hypothesis is still true in this scheme -- this is because $E(R_{js})$ is the same for all samples, which means that the average cell composition is the same between groups.

As all cells were sampled from the same pool, the null hypothesis of constant abundance should be true for each hypersphere.
Thus, $p$-values from both methods should be uniformly distributed.
We calculated the observed type I error rate as the proportion of $p$-values below a specified threshold $\alpha=0.01$ or 0.05.
For edgeR, we found that the observed type I error rate was close to or below the specified threshold for all simulation schemes and threshold values (Supplementary Figure~\ref{fig:testtest}a).
Accurate type I error control indicates that the specificity of edgeR is maintained when applied to counts of cells assigned to hyperspheres.
edgeR also routinely yields lower $p$-values than the Mann-Whitney test for hyperspheres with extreme log-fold changes in abundance between conditions (Supplementary Figure~\ref{fig:testtest}b).
This is due to the loss of power from using ranks with small sample sizes in the latter.
edgeR is more sensitive as its parametric model accounts for the size of the change in abundance, yielding smaller $p$-values at the same log-fold changes.
These results suggest that edgeR is appropriate for detecting differential abundance in mass cytometry data.
\end{color}

\section{Controlling the spatial FDR}

\subsection{A detailed explanation of the spatial FDR}
\label{sec:fdr}
Let us split the $M$-dimensional space into arbitrarily small non-overlapping partitions of similar volume.
For example, these partitions might be pixels in two-dimensional space, or voxels in three-dimensional space.
Each hypersphere is assigned to the partition containing its median-based position.
The outcome of the test for differential abundance for the hypersphere is used as a proxy for the outcome of its assigned partition (Supplementary Figure~\ref{fig:fdrdemo}).
In the simplest scenario, a partition containing only one hypersphere will be represented by that hypersphere.
If multiple hyperspheres are assigned to a partition, a single hypersphere is randomly selected as the representative.
We define the FDR across the expected set of representative hyperspheres (defined as the set of all hyperspheres, where each hypersphere is weighted by the probability of being sampled from its partition) as the spatial FDR.
By controlling this value below a specified threshold, we effectively control the FDR across partitions, and thus, the FDR across the volume of those partitions.

In practice, we do not need to explicitly define partitions in order to control the spatial FDR.
Rather, the probability of sampling a representative hypersphere from a partition is inversely proportional to the density of assigned hyperspheres in that partition.
Given that our definition above considers small partitions of similar size, the local density of each hypersphere can approximate the density of the partition to which it is assigned.
Thus, instead of directly allocating hyperspheres into partitions and computing the partition density (which would involve arbitrary definitions of the shape, size and arrangement of partitions), we compute the local density of each hypersphere by applying a kernel density estimator to the positions of all tested hyperspheres.
The weight of each hypersphere is defined as the reciprocal of its local density.
This approximates the relative probability (scaled by some constant, which can be ignored) of sampling that hypersphere as a representative of its partition.
Finally, the Benjamini-Hochberg method is applied to the hypersphere $p$-values with the associated weights.
This controls the FDR across the expected set of representative hyperspheres.

% The bandwidth parameter is analogous to the size of the partitions.
% In general, it must be small enough so that the the hyperspheres within each hypothetical partition are well-correlated.
% Otherwise, the FDR over the expected set would not approximate the expected FDR over all possible sets.
% One might argue that larger partitions are more relevant to interpretation, given that people generally won't pore over every subspace.
% However, if you increase the bandwidth, you'll eventually end up with all weights being equal, i.e., naive BH.
% The bandwidth here is chosen based on nearest-neighbour considerations, so it will shrink with an increasing number of cells.
% In other words, you'll get a more precise estimate of the density, or a more precise definition of the volume around the DA hyperspheres.

We stress that the aforementioned partitions are only necessary for the theoretical definition of the spatial FDR.
There is no need to actually construct these partitions when controlling the FDR in real analyses.

As described in Figure~\ref{fig:overview}d, the spatial FDR can be roughly interpreted as the proportion of the volume occupied by DA hyperspheres that corresponds to false positives.
This is based on considering partitions of similar size to the hyperspheres, such that the total volume of the partitions, and the proportion of which is false positive, is similar to that of the hyperspheres.
(The corresponding assumption in our control procedure would be that the kernel bandwidth is similar to the hypersphere radius.
However, the results are quite robust to the choice of bandwidth -- see Supplementary Table~\ref{tab:param} -- so we will ignore this subtlety.)
While a definition of the spatial FDR based on the hypersphere volumes is intuitive, it is difficult to implement.
Computing the total volume of (overlapping) hyperspheres in high-dimensional space is not straightforward.
It is also unclear how to define the ``false positive volume'' in the presence of overlaps between a false and true positive hypersphere.
Our definition of the spatial FDR, while less intuitive, is easier to control.

\subsection{Applying the weighted Benjamini-Hochberg procedure}
Let each null hypothesis $i$ be associated with a $p$-value $p_{(i)}$ and a weight of $w_{(i)}$.
Assume that there are $n$ null hypotheses, ordered such that $p_{(1)} < p_{(2)} < ... < p_{(n)}$.
To control the FDR at some threshold $\alpha$, a weighted BH procedure is applied \cite{benjamini1997multiple} to reject any null hypothesis where the $p$-value is less than the threshold
\[
    \max_i \left\{ p_{(i)} : p_{(i)} \le \alpha \frac{\sum_{l=1}^{i} w_{(l)}}{\sum_{l=1}^{n} w_{(l)}} \right\}  \;.
\]
%Intuitively, this can be understood as treating the weighted hypothesis $i$ as a collection of $w_{(i)}$ unweighted hypotheses, and applying the standard BH method to the total set of unweighted hypotheses across all $i$.
%(This reasoning is still applicable for non-integer $w_i$, as such values can be scaled up to yield values that are arbitrarily close to integers.
%The calculation of $T$ is unaffected as any scaling will cancel out.
%Moreover, while the unweighted hypotheses in each collection are completely dependent, this should not compromise FDR control as the BH method is robust to dependencies \cite{reiner2003identifying,kim2008effects}.)

To control the spatial FDR, we apply the weighted BH method to the hypersphere statistics.
Each hypersphere corresponds to one null hypothesis, while its weight is defined as the reciprocal of its local density.
Here, a decision must be made regarding the choice of kernel density estimator and bandwidth.
For the latter, we compute the distance from each hypersphere position to its 50\textsuperscript{th} nearest neighbour.
The bandwidth is defined as the median of this distance across all hyperspheres.
This ensures that, on average, around 50 neighbours will be available to stably calculate the local density for each hypersphere.
We also use a tricube kernel to provide some robustness to the choice of bandwidth (Supplementary Table~\ref{tab:param}).
This gives more weight to closer neighbours while reducing the influence of cells that fall just inside the bandwidth.

\subsection{A discussion on the relevance of the spatial FDR}
An obvious question is, why we do not directly count cells into equally sized and spaced partitions, rather than using hyperspheres?
This would certainly simplify FDR control as the BH method could be applied directly to the $p$-values for the partitions.
However, choosing a value for the spacing parameter is not straightforward.
A value that is too small would be computationally impractical, while a value that is too large will sacrifice spatial resolution.
Our approach avoids this problem by using hyperspheres centred on cells.
This means that a hypersphere will generally be present at relevant parts of the high-dimensional space (i.e., those occupied by cells), while limiting the total number of hyperspheres to be proportional to the number of cells.

It is worth noting that we control the FDR in the expected set of representative hyperspheres, rather than the expectation of the FDR with respect to all possible sets of randomly sampled representatives.
Formally speaking, control of the FDR would refer to controlling the latter value.
The FDR across the expected set is used in our analysis because it is simpler to compute.
It also approaches the expected FDR when considering small partitions, as strong correlations betwen hyperspheres in the same partition mean that the expected set will have a similar (frequency-weighted) distribution of $p$-values as any instance of the sampled set.

In terms of interpretation, controlling the spatial FDR may not be the same as controlling the FDR across the underlying subpopulations.
For example, consider a scenario containing a DA subpopulation of large volume and a non-DA subpopulation of small volume.
Assume that both subpopulations are detected after the DA analysis.
In this situation, the observed spatial FDR would be small as the proportion of volume taken up by the second (false positive) subpopulation is low.
By comparison, the observed FDR across subpopulations would be larger (50\%) because one of the two subpopulations is a false positive.
The FDR across subpopulations is appealing as it is more intuitive than the spatial FDR.
However, it is difficult to control in general as it relies on the subpopulations being well-defined.
Furthermore, the use of the spatial FDR is justified by the potential presence of further substructure within the larger DA subpopulation.
This necessitates separate examination of each part of the first subpopulation, proportional to its volume.

% Consider an extreme case where all subpopulations reach the lower limit of size (i.e., technical + biological variability only, no substructure).
% In such cases, control of FDR over volume would be irrelevant; you would never look at part of these subpopulations, because they can't be substructure.
% However, in such cases, the spatial FDR would be roughly equivalent to controlling the FDR from the combined p-value across subpopulations.
% To illustrate, for a small subpopulation, all hyperspheres would have the same or similar p-values due to strong correlations.
% The combined p-value per subpopulation (via Simes' method) would then be pretty similar to each hypersphere p-value.
% The FDR would then be controlled across subpopulations using their combined p-values.
% If we use the weighted FDR - again, because the p-values are similar for each subpopulation, they would form a more-or-less contiguous block in the p-value ranks.
% Now, hyperspheres from each subpopulation would have the same total weight (as each subpopulation is of the same volume).
% This means that the final adjustment of the weighted individual p-values would generally be similar to that for the combined unweighted p-values.

A similar issue arises from the fact that visualization and interpretation of the differential subspaces is performed in low dimensions, e.g., with PCA or $t$-SNE plots.
There is no guarantee that the FDR across, say, pixels in the two-dimensional space is equal to the spatial FDR computed in the original $M$-dimensional space.
One could overcome this problem by controlling the spatial FDR using hypersphere coordinates in low-dimensional space.
However, this level of statistical rigour seems unnecessary for data exploration.
In addition, one of the aims of the DA analysis is to simplify data visualization by only processing the subset of DA hyperspheres.
It is not clear how the FDR can be rigorously controlled across the low-dimensional space if dimensionality reduction is performed on hyperspheres that have been pre-selected for significance.

We note that the concept of the spatial FDR is analogous to the FDR across areas \cite{pacifico2004false} or the size-weighted FDR across clusters \cite{benjamini2007false} used in analyses of functional magnetic resonance imaging (fMRI) data.
fMRI data analysis is a similar problem in that the signal of interest has a spatial component (2-3 dimensions) and the aim is to control the FDR across this space.
However, the application of these methods to hypersphere-based $p$-values is not obvious -- we do not use random fields for hypothesis testing, and we explicitly avoid clustering by using hyperspheres.
This motivated the development of our own spatial FDR-controlling procedure.

\begin{color}{red}
\subsection{Assessing the use of weights for spatial FDR control}
We performed simulations based on the MEF reprogramming data to test the use of density-based weights.
Each sample was constructed using the weighted sampling scheme described in Section~\ref{sec:edgeRsim}, for an experimental design containing several replicates in each of two groups.
We then added a further $T_s/10$ cells to each sample $s$, where $T_s$ is the original number of cells in $s$.
The additional cells were assigned marker intensities of zero for all samples in the first group, and intensities of 1 for all samples in the second group.
This represents a differential subpopulation between groups where a subpopulation at $(0,0, \ldots, 0)$ is lost and a subpopulation at $(1, 1, \ldots, 1)$ is gained.
While more complex differential events can be simulated, we use this simple set-up as we are not interested in the true differences, but rather, their effect on the detection of false positives.

We applied the BH method to the $p$-values for all hyperspheres, either directly or with density weights.
Detected DA hyperspheres were defined as those with adjusted $p$-values below 0.05.
To measure the observed spatial FDR across the detected hyperspheres, we partitioned the $M$-dimensional space into non-overlapping hypercubes with side lengths ranging from 0.2 to 1.
Each DA hypersphere was assigned to the hypercube containing its median-based position.
For each non-empty hypercube, we computed the proportion of its assigned hyperspheres that were not truly differential.
The observed spatial FDR was defined as the mean of these proportions across all non-empty hypercubes.
(This reflects the definition in Section~\ref{sec:fdr}.
Each hypercube represents a partition of similar volume, from which one hypersphere is chosen as a representative.
The mean proportion is an estimate of the expected proportion of representatives that are false positives.)

In our simulations, the BH method with density weights was able to control the observed spatial FDR close to or below the specified threshold (Supplementary Figure~\ref{fig:fdr}).
In comparison, na\"ive application of the BH method (i.e., without weighting) failed to control the spatial FDR.
This is because the na\"ive approach controls the FDR across hyperspheres, which is generally not equivalent to controlling the FDR across volume (Figure~\ref{fig:overview}d).
The weighting approach performs better as it explicitly controls for the latter feature.
\end{color}

\section{Detailed interpretation of non-redundant hyperspheres}
While dimensionality reduction is useful for providing an overview of the data, it necessarily discards information that may actually be biologically relevant.
To assist users with more detailed interpretation of individual hyperspheres, we provide an option to prune out redundant hyperspheres.
First, hyperspheres are sorted by their $p$-value.
A hypersphere is considered non-redundant if its position is more than 1 intensity unit away (in one or more dimensions) from another non-redundant hypersphere with a lower $p$-value.
One unit represents an approximately 10-fold change in marker intensity between hyperspheres, which is large enough to warrant separate examination of those hyperspheres, though users can adjust this threshold as desired.
The non-redundancy criterion is evaluated for all hyperspheres in order of increasing $p$-value, which ensures that DA hyperspheres are reported as non-redundant rather than their neighbouring non-DA counterparts.
This procedure yields a small number of non-redundant hyperspheres that can be examined individually.
For example, over 7000 hyperspheres were detected as significantly DA in the Oct4-GFP reprogramming time course, but only 325 were considered to be non-redundant.
These can be investigated using a simple user interface like that shown in Supplementary Figure~\ref{fig:shiny}.
This provides a clear view of the median intensity of each marker and facilitates the identification of the biological subpopulation represented by each hypersphere.

\begin{color}{red}
\section{Description of the MEF reprogramming study}

\subsection{Overview}
We demonstrate our approach using data from a study of mouse embryonic fibroblast (MEF) reprogramming \cite{zunder2015continuous}.
In this study, primary and secondary MEFs were reprogrammed to induced pluripotent stem cells.
Primary MEFs expressing green fluorescent protein (GFP) from the endogenous Oct4 locus (Oct4-GFP) were transduced with lentiviruses expressing a doxycycline-inducible suite of reprogramming factors.
Secondary MEFs expressing either GFP or a neomycin resistance gene (Neo) from the endogenous Nanog locus (Nanog-GFP or Nanog-Neo, respectively) already contained doxycycline-inducible reprogramming transgenes.
For each MEF reprogramming system, a time course was constructed by taking samples at 13-15 timepoints between days 0 and 20 (for Oct4-GFP) or 30 (for Nanog-GFP or Nanog-Neo) after doxycycline-induced transgene expression.
All samples from each time course were barcoded, stained with metal-conjugated antibodies and profiled by mass cytometry.
The aim of the original data analysis -- and of our re-analysis -- was to detect subpopulations that change in abundance over time within each reprogramming system.
\end{color}

\newcommand{\hi}{\textsuperscript{high}}
\newcommand{\lo}{\textsuperscript{low}}

\subsection{Annotating subpopulations in the Oct4-GFP time course}
To annotate Figure~\ref{fig:oct4} in the main text, we examined the marker intensities of each characterised subpopulation in Figure~3D of Zunder \emph{et al.} \cite{zunder2015continuous} and identified the cluster with the most similar intensities in our Supplementary Figure~\ref{fig:oct4_markers}.
The important markers for each subpopulation are listed below:
\begin{itemize}
    \item MEFs were identified as Thy1\hi{}mEF-SK4\hi{}Cd140a\hi{} and Oct4\lo{}Sox2\lo{} along with lower expression of Klf4 relative to other subpopulations.
        This was visually supported by the fact that the same cells were pS6\hi{}\textbeta-Catenin\hi{}I\textkappa{}B\textalpha\hi{} in the original figure.
    \item The OSKM non-expressing population was identified as Oct4\lo{}Sox2\lo{} with lower Klf4.
        In addition, a majority of cells exhibited lower Thy1 and Cd140a expression compared to the neighbouring MEF population.
        This was further supported by low expression of pS6 in a majority of these cells, as well as reduced \textbeta-Catenin and I\textkappa{}B\textalpha{} expression and higher p53 expression relative to MEFs.
%    \item The transition between MEFs and OSKM non-expressing cells was identified as pS6\hi{}, with moderate Thy1, mEF-SK4 and Cd140a expression.
%\textbeta-Catenin and I\textkappa{}B\textalpha{} levels were lower than in MEFs.
    \item A small subset of MEFs in Figure~\ref{fig:oct4} exhibited very high Cd140a expression.
This represents OSKM non-expressing cells that reverted to a MEF-like endpoint state after doxycycline withdrawal.
    \item The Oct4\hi{}Klf4\hi{} population was identified as it was named, supported by high expression of Sox2.
    \item The SC4-like population was identified as Klf4\hi{}Oct4\hi{}Cd73\hi{}, additionally supported by high expression of Ki67 and low expression of Sox2.
    \item Cells undergoing mesenchymal-epithelial transition were identified as EpCAM\hi{}, supported by high expression of both \textbeta-Catenin and Oct4.
    \item Ki67\lo{} reverting cells were identified as Ki67\lo{}Cd73\hi{}.
        This annotation was supported by high expression of mEF-SK4 in parts of the population.
    \item The Nanog\hi{} trajectory in the original study was a continuum of Nanog expression, starting from Nanog\lo{} cells and progressing to Nanog\hi{} cells.
        In our re-analysis, it manifests in Supplementary Figure~\ref{fig:oct4_markers} as several Nanog-intermediate subpopulations close to the Lin28\hi{} and embryonic stem cell (ESC)-like subpopulations.
        The annotation was supported by high expression of EpCAM and SSEA1, as well as higher pS6 expression in some parts of the trajectory.
    \item ESC-like cells were identified as the population with the highest expression of Nanog and high expression of SSEA1 and Lin28.
        This was supported by high levels of H3K9ac, H4Kac, Cd54, pSrc and pERK.
    \item Lin28\hi{} cells were identified as named, supported by high expression of Cd24.
They were further distinguished from ESC-like cells by having higher expression of Cd140a and lower expression of Nanog.
    \item The Ki67\hi{} population was identified as named, supported by lower expression of Oct4.
    \item The mixed 4F population from the original study was highly heterogeneous and difficult to characterize.
We identified it as mostly Klf4\lo{}, containing subpopulations with intermediate c-Myc expression and both high and low Sox2 expression.
However, we were unable to identify the Oct4\lo{} subpopulations.
\end{itemize}
\begin{color}{red}
In summary, we were able to detect most of the previously defined subpopulations as being DA over the time course.
This included intermediate populations as well as the reprogramming end points, i.e., the ESC-like cells, the mesendoderm-like Lin28\hi{} cells, and Thy1\hi{}mEF-SK4\hi{} cells that likely failed to reprogram and instead reverted to a MEF-like phenotype.
In particular, the abundance of MEFs dropped over time while the abundance of ESC-like cells increased, consistent with the effects of reprogramming.
Many of these subpopulations were further resolved into distinct subsets based on specific markers such as IdU.

We also identified changes in abundance of subpopulations that were not explicitly classified by Zunder \emph{et al.}
This included an increase in potentially apoptotic cells with cleaved Caspase-3;
a non-linear change in abundance of a subpopulation of SC4-like cells with phosphorylated STAT3, AMPK and PLK1 (Supplementary Figure~\ref{fig:nonlinear});
and a decrease in abundance of a subpopulation of cells simultaneously expressing high levels of the MEF marker Thy1 along with the reprogramming factors Sox2 and Oct4.
Thus, our DA analysis method is able to identify significant changes in abundance even in small or transitional subpopulations.

\subsection{Changes at specific time points}
We also examined changes in abundance at critical junctures in the time course.
Supplementary Figure~\ref{fig:importanttime} shows the effect of doxycycline-induced transgene expression and doxycycline withdrawal.
In the former, the MEFs decrease in abundance while the mixed 4F population begins to increase, consistent with induction of some of the reprogramming factors.
Upon withdrawal, the mixed 4F and SC4-like subpopulations decrease as cells start to progress towards one of the reprogramming endpoints.
This highlights the flexibility of our analysis, where the data can be easily interrogated to study changes at specific time points of interest.

\subsection{Comparison to the Zunder \textit{et al.} analysis}
Our analytical pipeline offers several advantages over the original analysis of Zunder \emph{et al.} \cite{zunder2015continuous}.
The latter is mainly descriptive, whereas the error-controlling procedures in our analysis provide a greater degree of confidence as to whether the detected changes in abundance are genuine.
Figure~\ref{fig:oct4} also uses both visual dimensions to separate subpopulations, while the original analysis reserves one dimension for time.
%This allowed us to detect a novel subpopulation that appeared after induction and expressed both MEF markers and reprogramming factors, suggesting that there may be coexpression of these proteins in early stages of reprogramming.
This enhances resolution of distinct subpopulations, albeit at the cost of temporal resolution.
Of course, our approach is modular so alternative visualization schemes can be easily applied (on the significant hyperspheres, or the cells contained within them) to focus on particular aspects of the data.
For example, methods like SPADE can be applied to organize the detected DA hyperspheres into a tree for visualization of lineages.
\end{color}

\section{Identifying subpopulations in the BMMC data set}
\label{sec:bmmc}
Levine \textit{et al.} \cite{levine2015datadriven} examined bone marrow aspirates from healthy donors and acute myeloid leukemia patients under various brief stimulation conditions.
To demonstrate the general applicability of our method, we performed a limited re-analysis of this data set, with the aim of identifying changes in subpopulation abundance upon IL-10 treatment in the five healthy donors only.
We obtained the raw FCS files for the relevant samples from Cytobank (accession 44185).
Pre-processing of the marker intensities was performed as described for the MEF reprogramming data set, with some modifications.
Specifically, cells with high outlier values for the viability marker were gated out to remove dead cells, and intensities were range-normalized across batches as described above.
We then used our pipeline to test for differences in abundance between each treated sample and its donor-matched untreated control.
Here, counts were modelled using an additive design of treatment with a donor blocking factor.
\revised{We also increased $r_0\sqrt{2}$ to 0.55 to ensure that most hyperspheres contained at least 15-20 cells, based on a plot similar to Figure~\ref{fig:nvd}.
Finally, we ran $t$-SNE on the significant hyperspheres with positive and negative log-fold changes separately to improve the clarity of the visualization.}

Our DA analysis revealed several differential subpopulations at a spatial FDR of 5\% -- namely, a CD11b\hi{} CD64\hi{} population of monocytes\revised{, a CD19\hi{} population of B cells and a CD3\hi{} population of T cells (Supplementary Figures~\ref{fig:bmmc}-\ref{fig:bmmc_markers}).
Within each subpopulation, pSTAT3\hi{} hyperspheres increased in abundance upon stimulation compared to the untreated samples, while pSTAT\lo{} hyperspheres descreased in abundance.
This is consistent with the expected effects of IL-10 on mature immune cells.}
Similar to Levine \textit{et al.}, we found that, within the myeloid lineage, only mature cellular populations and not progenitor bone marrow populations responded to IL-10 stimulation, i.e., significant differences were not detected for CD34\hi{} CD3\lo{} CD11b\lo{} CD19\lo{} hyperspheres.
We also identified some new subpopulations, such as a subset of CD3\hi{} cells that responded to IL-10 stimulation with increased pSTAT3 but did not express the T lineage marker CD7.
Thus, our method is able to provide statistical rigour as well as additional biological insight.

\begin{color}{red}
\section{Comparing to approaches based on clustering}

\subsection{Overview}
As mentioned earlier, existing methods for analyzing mass cytometry data involve an initial clustering step.
This approach can be easily extended to DA analyses, where the number of cells in each cluster from each sample is counted, and the counts for each cluster are tested for differences between conditions.
We compare our hypersphere-based method against two cluster-based approaches -- a custom approach involving hierachical clustering of cells followed by testing with edgeR; and the CITRUS software \cite{bruggner2014automated}, which uses statistical methods developed for microarrays \cite{tusher2001significance}.
While testing for changes in abundance within clusters is intuitive, it is subject to the performance of the clustering algorithm \cite{kerr2001bootstrapping,ronan2016avoiding}.
This is especially relevant for subpopulations that are not clearly separated from each other.
For example, if a DA subpopulation is incorrectly clustered with a non-DA subpopulation, any change in the former will be masked by the latter.
This will compromise detection power of the subsequent DA analysis.
Cluster formation also depends upon the choice of algorithm and parameters \cite{datta2003comparisons,wiwie2015comparing}, which complicates the assessment of cluster reliability.

\subsection{Simulation design and analysis}
We simulated data for an experimental design involving 30 markers and two replicates in each of two conditions.
For a population of 20000 cells, we sampled intensities for each marker from a Normal(1, 0.6) distribution.
Each of these cells was randomly allocated to a sample, producing a non-DA population with equal representation in both conditions.
We also simulated intensities for two subpopulations of 40 cells each, where the cells from each subpopulation were allocated to samples in one condition only.
Marker intensities for cells in these subpopulations were sampled from a Normal($x$, 0.3) distribution, where $x=4$ for the first marker in the first subpopulation and $x=2$ otherwise.
This yields two small DA subpopulations that lie adjacent to each other but change in opposite directions between conditions (Supplementary Figure~\ref{fig:clustersim}).

To perform the custom cluster-based DA analysis, all cells were used for complete-linkage hierarchical clustering based on the Euclidean distances between cells in the $M$-dimensional space.
Clusters were defined by cutting the dendrogram with the cutree command in R to generate 5-500 clusters.
For each cluster, the number of cells from each sample was counted.
These counts were analyzed in edgeR to identify clusters with significant differences between conditions, as previously described.
Correction for multiple testing was performed by directly applying the BH method to the cluster-level $p$-values.
Detected clusters were defined at an FDR of 5\%.
To run CITRUS (v0.08), the citrus.full command was used with family set to ``classification'', featureType set to ``abundances'' and modelType set to ``sam''.
Downsampling was performed to 1000 cells per sample and the minimum cluster size was set to 5\%, as recommended.
Detected clusters were defined at an FDR of 5\%, as reported by the SAM method.
For our hypersphere-based method, the DA analysis was performed as previously described and hyperspheres were detected at a spatial FDR of 5\%.

For the cluster-based methods, the centre of each cluster in $M$-dimensional space was defined from the median intensity across its cells for each marker.
We use the cluster centre as a summary of the location of the entire cluster, as this reflects the common use of the median marker intensity to characterise cell clusters in practical applications \cite{qiu2011extracting,bruggner2014automated}.
Each simulated DA subpopulation was considered to be successfully detected if the centre of a detected cluster was within $0.5\sqrt{M}$ of the true subpopulation centre.
This assessment was repeated using the median-based positions of DA hyperspheres.
For each method, we computed the percentage of simulation iterations in which each DA subpopulation was successfully detected.

We observed that these subpopulations were consistently detected as being differentially abundant by our hypersphere-based method but not by most of the cluster-based methods (Supplementary Figure~\ref{fig:clustersim}).
This is because clusters cannot be unambiguously defined in this scenario, such that a cluster corresponding to one of the DA subpopulations will include cells from the other subpopulation.
Subsequently, the power to detect this cluster is reduced as the DA log-fold change in one direction is weakened by the contribution from the subpopulation changing in the other direction.
This is likely to be problematic in situations where subpopulations are not clearly separated.
Examples include gating strategies commonly used in immunophenotyping to characterise subsets of T and B cells \cite{finak2016standardizing}, as well as protocols for isolation of haematopoietic stem cell and progenitor populations \cite{wilson2015combined}.
Indeed, many of the subpopulations in the MEF data set form a continuous trajectory over time \cite{zunder2015continuous}, so suboptimal DA detection due to ambgiuous cluster formation is not surprising (Supplementary Figure~\ref{fig:clusterreal}).
In such cases, the use of hyperspheres may be more appropriate because each subspace is tested for differences, even if the underlying biological subpopulations are poorly defined.
\end{color}

\subsection{Issues associated with overclustering}
As Supplementary Figure~\ref{fig:clustersim} demonstrates, the use of a large number of clusters (i.e., ``over-clustering'') can mitigate the disadvantages of cluster-based methods compared to hyperspheres.
At a certain number of clusters, the size of each cluster will be roughly similar to that of each hypersphere.
This improves the spatial resolution of clustering and increases the power to detect small DA subpopulations.
However, the need to explicitly define cluster boundaries still leads to some loss of power relative to our hypersphere-based method, particularly in cases where the separation between DA and non-DA subpopulations is not obvious.

There are also some practical issues with the use of over-clustering in routine analyses.
The most obvious problem is an appropriate choice of the number of clusters $k$.
If $k$ is too small, resolution is lost, while if $k$ is too large, the counts per cluster will be too low to reject the null hypothesis.
In some respects, this is analogous to the choice of the hypersphere radius $r$.
However, $r$ scales with the number of markers and is largely agnostic to the true number of subpopulations in the data set.
This is not the case with $k$, e.g., a choice of $k=50$ in a data set with 10-20 subpopulations defined by 10 markers may not be sufficient when analyzing a more heterogeneous data set with 30 markers defining hundreds of distinct subpopulations.

While the DA analyses can be repeated with multiple $k$, integrating these analyses into a single set of results is challenging.
This is due to the redundancies and dependencies between different $k$, which affects FDR control and interpretation of the results.
For example, if a cluster is detected as DA, and all its internal subclusters are also detected, how many discoveries does that actually constitute?
If some of the subclusters are false positives, what would the FDR be?
Should all, some or none of the subclusters be reported and visualized?
Even with a single $k$, the FDR across clusters is not easily interpretable after over-clustering, because we cannot assume that each cluster represents (accurately or otherwise) some biologically meaningful subpopulation.
Rather, the interpretation would become closer to that of the spatial FDR, possibly requiring some additional work to explicitly control the spatial FDR if the volume of each cluster is not the same.

\begin{color}{red}
\section{Something something signalling}
A complementary approach to differential analyses is to detect differential marker expression within the same subpopulation.
Given a (manually or automatically defined) subpopulation, the average intensity of a particular marker can be compared between conditions \cite{anchang2016visualization,behbehani2015mass}.
This detects changes in the expression of activation or signalling markers within a subpopulation.
Our pipeline is different in that it tests for changes in the abundance of cells, rather than their marker intensities.
However, these two types of differential events are closely related.
Consider a marker $X$ in subpopulation $Y$ that increases in expression between two conditions. 
In our analysis, this will manifest as the appearance of new subpopulation $Y'$, separated from $Y$ in the high-dimensional space by an increase in the intensity of $X$.
This new subpopulation will then be detected as DA between conditions.
We observed this effect in our re-analysis of the BMMC data set, where an increase in pSTAT3 levels upon IL-10 stimulation manifested as an increase in the abundances of pSTAT3\textsuperscript{high} subpopulations and a concomitant decrease in the abundances of pSTAT3\textsuperscript{low} subpopulations.
Thus, changes in intensity can be interpreted as changes in abundance for detection with a DA analysis pipeline.
\end{color}

\bibliography{ref}
\bibliographystyle{unsrt}

%%%% FIGURES BEGIN HERE %%%%
\newpage

\begin{figure}[p]
    \begin{center}
        \includegraphics[width=0.32\textwidth]{../real/neighbours/Cytobank_43324_4FI.pdf}
        \includegraphics[width=0.32\textwidth]{../real/neighbours/Cytobank_43324_NG.pdf}
        \includegraphics[width=0.32\textwidth]{../real/neighbours/Cytobank_43324_NN.pdf}          
    \end{center}
    \caption{
        Euclidean distance from each cell to its nearest neighbours as a function of the number of markers.
        For each cell in the first sample of each time course in the MEF reprogramming study, its nearest neighbours were identified in the full (36-dimensional) space.
        A subset of 9-25 markers were randomly chosen and the distance to each nearest neighbour was recalculated for each cell in the reduced space.
        The 10\textsuperscript{th} nearest neighbour was used for Oct4-GFP and Nanog-Neo, while the 1\textsuperscript{st} nearest neighbour was used for Nanog-GFP.
        Each point represents the mean distance across all cells for a given number of markers, while the error bar represents the sample standard deviation across cells.
        This was also repeated using all 36 markers.
        The red line marks the hypersphere radius that is used for each number of markers.
    }
    \label{fig:radius}
\end{figure}

\begin{figure}[p]
    \begin{center}
        \includegraphics[width=0.6\textwidth]{../real/neighbours/nvd_Cytobank_43324_4FI.pdf}
    \end{center}
    \caption{The dependence of the hypersphere radius (based on the choice of $r_0\sqrt{2}$) on the cell count for each hypersphere in the Oct4-GFP time course.
        For each hypersphere centred on a cell, the radius required to include a certain number of nearest neighbours is computed.
        The distribution of radii across all hyperspheres is shown as a boxplot for each neighbour.
        The red line represents the default $r_0\sqrt{2}=0.5$.
        The blue lines mark the $r_0\sqrt{2}$ corresponding to median distances for including 7 and 29 neighbours.
    }
    \label{fig:nvd}
\end{figure}

\begin{table}[btp]
\caption{Effect of changes to the hypersphere radius on the DA analysis.
The number of hyperspheres retained after filtering is shown, along with the number detected with significant differences at a spatial FDR of 5\%.
In each altered analysis, the number of DA hyperspheres gained or lost relative to the original analysis is reported.
The effect of altering the bandwidth in the spatial FDR calculation was also tested, by defining the bandwidth using the 20\textsuperscript{th} (smaller) and 100\textsuperscript{th} nearest neighbour (larger).
}
\label{tab:param}
\begin{center}
\begin{tabular}{l l r r r r r}
\hline
\textbf{Dataset} & \textbf{Statistic} & \textbf{Original} & \multicolumn{2}{c}{\textbf{Value of $r_0\sqrt{2}$}} & \multicolumn{2}{c}{\textbf{Bandwidth}} \\
                 &                    &          & \makebox[0.4in][r]{\textit{0.48}} & \makebox[0.6in][r]{\textit{0.52}} 
                                                 & \makebox[0.6in][r]{\textit{Smaller}} & \makebox[0.6in][r]{\textit{Larger}} \\
\hline
Oct4-GFP & Total & 7720 & 4984 & 10799 & 7720 & 7720 \\
 & Significant & 7416 & 4837 & 10242 & 7418 & 7414 \\
 & Gained & - & 35 & 2916 & 2 & 0 \\
 & Lost & - & 2614 & 90 & 0 & 2 \\
\hline
Nanog-GFP & Total & 6297 & 4137 & 8700 & 6297 & 6297 \\
 & Significant & 5947 & 3917 & 8291 & 5947 & 5944 \\
 & Gained & - & 63 & 2389 & 0 & 0 \\
 & Lost & - & 2093 & 45 & 0 & 3 \\
\hline
Nanog-Neo & Total & 22043 & 15025 & 28944 & 22043 & 22043 \\
 & Significant & 21532 & 14663 & 28271 & 21532 & 21532 \\
 & Gained & - & 53 & 6809 & 0 & 0 \\
 & Lost & - & 6922 & 70 & 0 & 0 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{figure}[tbp]
    \begin{center}
        \includegraphics[width=0.8\textwidth,trim=30mm 50mm 20mm 50mm,clip]{../simulations/radius_setup.pdf}
    \end{center}
    \caption{Schematic of the simulation design used to examine the effect of increasing the hypersphere radius.
        Here, the experimental design consists of two samples from different conditions and 30 markers.
        Two subpopulations are present in the high-dimensional space, one of which is DA between samples (red) and the other is not (grey).
        Cells from each subpopulation (10000 each) are uniformly distributed within a 30-dimensional sphere with radius (referred to as ``size'', above) of length $0.5\sqrt{30}$, centred at the subpopulation mean (crosses).
        The means of the two subpopulations are separated by a distance of 0.5 in each dimension.
        Each hypersphere's position is calculated by taking the median in each dimension across all cells in the hypersphere.
        Note that the radii of the hyperspheres and subpopulations need not be identical.
    }
    \label{fig:radius_schematic}
\end{figure}

\begin{figure}[tbp]
    \begin{center}
        \includegraphics[width=\textwidth]{pics/Figure_RadiusSim.pdf}
    \end{center}
    \caption{Simulation results demonstrating the effect of expanding the radius on the median-based position of each hypersphere.
        For each hypersphere, the distance between its position and the mean of the DA subpopulation was computed and plotted against the log-fold change in abundance between samples.
        Hyperspheres were coloured based on whether they were centred on cells from the DA (red) or non-DA (grey) subpopulations.
        The vertical dashed line marks the distance corresponding to the size of the DA subpopulation, while the horizontal dashed line represents a $\log_2$-fold change of 1.
        This process was repeated to create plots for a range of increasing hypersphere radii and for different simulation scenarios.
        Scenario 1 refers to the default scenario described in Supplementary Figure~\ref{fig:radius_schematic}, while scenario 2 involves decreasing the number of cells in the DA subpopulation to 5000; scenario 3 increases the distance between subpopulations to 0.8 in all dimensions; and scenario 4 decreases the size of the DA subpopulation to $0.2\sqrt{30}$.
    }
    \label{fig:radius_position}
\end{figure}

\begin{figure}[tbp]
    \begin{center}
        \includegraphics[width=0.32\textwidth,page=6,trim=0mm 0mm 0mm 20mm,clip]{../refdata/pics/ecdf_raw2.pdf}
        \includegraphics[width=0.32\textwidth,page=6,trim=0mm 0mm 0mm 20mm,clip]{../refdata/pics/ecdf_rnorm2.pdf}
        \includegraphics[width=0.32\textwidth,page=6,trim=0mm 0mm 0mm 20mm,clip]{../refdata/pics/ecdf_wnorm2.pdf}
    \end{center}
    \caption{\revised{Histograms of phosphorylated STAT3 intensities of BMMCs from five healthy, untreated individuals in the Levine \textit{et al.} data set, before normalization (left), after range-based normalization (middle) and after warping normalization (right).
        Each curve represents a separate individual and is labelled with a different colour.
        Points represent the proportion of cells with zero intensities in each batch.
    }
    }
    \label{fig:quantile}
\end{figure}

\begin{figure}[tbp]
    \begin{center}
        \includegraphics[width=0.49\textwidth,page=1]{../simulations/plot_shift.pdf}
        \includegraphics[width=0.49\textwidth,page=2]{../simulations/plot_shift.pdf}
    \end{center}
    \caption{Observed type I error rates in the simulation of intensity shifts, for DA analyses using either the default hypersphere radius ($r_0\sqrt{2}=0.5$) or an expanded radius designed to accommodate the estimated shift.
        Simulations were based on the MEF reprogramming time courses, using values sampled from a Normal distribution with mean zero and standard deviation of 0 (no shift) to 0.3 (moderate shift) to shift marker intensities between samples. 
        The observed type I error rate was calculated at a nominal threshold of 0.01 (left).
        The common dispersion estimate across all hyperspheres was also computed (right).
        All values represent the mean across 50 simulation iterations, with error bars representing the standard error.
    }
\label{fig:shift_sim}
\end{figure}

\begin{figure}[tbp]
    \begin{center}
    \includegraphics[width=0.6\textwidth]{../real/analysis/pics/dispersions.pdf}
    \end{center}
    \caption{NB dispersion estimates for all hyperspheres in each time course of the MEF reprogramming data set, plotted against the average number of cells across all samples in each hypersphere.}
    \label{fig:nbdisp}
\end{figure}

\begin{figure}[bt]
    \begin{center}
        \includegraphics[width=\textwidth]{../simulations/plot_alpha_power.pdf}
    \end{center}
    \caption{
        Performance of edgeR for detecting DA hyperspheres, using simulations based on the MEF reprogramming time courses.
        (a) Observed type I error rates with edgeR for each simulated time course with technical or biological variability, constructed by simple or weighted sampling of cells respectively.
        Bar heights represent the mean of 10 simulation iterations, with error bars representing standard errors.
        The red lines denote the specified thresholds of 0.01 (left) or 0.05 (right).
        (b) Log$_{10}$-ratios of the $p$-values computed by edgeR against those from the Mann-Whitney (MW) test, plotted against log$_{2}$-fold changes.
        Simulations were performed using weighted sampling for the Oct4-GFP time course, and each hypersphere was tested for differential abundance.
        Coloured points represent hyperspheres with absolute log-fold changes greater than two and $p$-values that are smaller in edgeR compared to the MW test (purple) or vice versa (orange).
        The number of such hyperspheres is also reported.
        The size of each point is determined by the edgeR $p$-value.
    }
    \label{fig:testtest}
\end{figure}

\begin{figure}[tbp]
    \begin{center}
        \includegraphics[width=\textwidth]{pics/Figure_FDRpartitions.pdf}
    \end{center}
    \caption{Assignment of hyperspheres to hypothetical partitions.
        Each point represents the median-based position of a hypersphere in a data set with two markers, coloured based on its test outcome
        (red for significant increases in abundance between conditions, blue for significant decreases, white for no significant differences, and grey for false positives).
        Each hypersphere is assigned to a partition, and the test outcome of each partition is defined as that of its hyperspheres.
        For simplicity, all hyperspheres in a partition have the same outcome here.
        The aim is to control the spatial FDR across partitions, rather than the FDR across hyperspheres.
        In this example, the spatial FDR is 25\% while the FDR across hyperspheres is 5\%.
}
    \label{fig:fdrdemo}
\end{figure}

\begin{figure}[bt]
    \begin{center}
        \includegraphics[width=0.9\textwidth]{../simulations/plot_FDR.pdf}
    \end{center}
    \caption{
        Control of the spatial FDR using the na\"ive and weighted BH methods, in simulations based on the MEF reprogramming time courses.
        The observed spatial FDR was calculated in $M$-dimensional space using hypercubes of width 0.2 to 1.
        Bar heights represent the mean of 50 simulation iterations, and the error bars represent standard errors.
        The red line denotes the nominal threshold of 5\%.
    }
    \label{fig:fdr}
\end{figure}

%\begin{figure}[tbp]
%    \begin{center}
%        \includegraphics[width=0.6\textwidth]{../simulations/FDR_setup.pdf}
%    \end{center}
%    \caption{\revised{A PCA plot of the positions of the detected hyperspheres in the FDR control simulation based on the Oct4-GFP time course.
%        Detected hyperspheres were defined using the na\"ive BH method to nominally control the FDR at 5\%.
%        Each hypersphere was assigned to a pixel of width 0.5 based on its coordinates in the two-dimensional space defined by the first two principal components.
%        Red pixels contain a majority of hyperspheres that are truly differential, while grey pixels contain only non-DA hyperspheres (i.e., false positives).
%        For the latter, the depth of colour is proportional to the number of hyperspheres.
%    } 
%    }
%\end{figure}

\begin{figure}[tbp]
    \begin{center}
        \includegraphics[width=\textwidth]{../real/analysis/pics/shiny.pdf}
    \end{center}
    \caption{Screenshot of the user interface for detailed interpretation of non-redundant hyperspheres.
            Each density curve represents the distribution of intensities across all cells for a particular marker.
            The red dot indicates the median intensity for the hypersphere being examined, while the interval contains 95\% of all cells in the hypersphere.
            The area under the curve is coloured depending on whether the median is high (yellow) or low (purple) compared to the intensities for the majority of cells.
            Each hypersphere can be annotated based on its intensities (e.g., to describe the subpopulation that it represents), and these labels are stored in memory for future use.
            In addition, the closest non-redundant hyperspheres that have already been labelled are shown for each new hypersphere, to simplify further labelling of nearby hyperspheres in the high-dimensional space.
            \revised{Navigation through the set of hyperspheres is done based on the hypersphere number or by selecting locations in a dimensionality reduction plot (in this case, $t$-SNE is used).}
}
\label{fig:shiny}
\end{figure}

\clearpage
%\newcommand{\bigfigopt}[1]{\includegraphics[width=\textwidth]{#1}}
\newcommand{\bigfigopt}[1]{\includegraphics[width=\textwidth,draft]{#1}}

\begin{figure}[p]
    \begin{center}
    \bigfigopt{../real/analysis/pics/Cytobank_43324_4FI_markers.png}
    \end{center}
    \caption{
        Marker intensities of differentially abundant subpopulations in the Oct4-GFP time course, detected at a spatial FDR of 5\%.
        Each plot corresponds to a marker while each point represents a hypersphere, coloured according to its median intensity for the corresponding marker.
        The boundaries of the colour range for each marker were set to the 1\textsuperscript{st} and 99\textsuperscript{th} percentiles of the intensities for all cells.
    }
    \label{fig:oct4_markers}
\end{figure}

\begin{figure}[p]
    \begin{center}
    \includegraphics[width=0.6\textwidth]{../real/analysis/pics/Cytobank_43324_NG_logFC.png}
    \end{center}
    \caption{
        Differentially abundant subpopulations in the Nanog-GFP time course, detected at a spatial FDR of 5\%.
        Each point is a hypersphere and is coloured according to its log-fold change in abundance over time.
        Grey points are hyperspheres with significant but non-linear changes in abundance.
    }
\end{figure}

\begin{figure}[p]
    \begin{center}
    \bigfigopt{../real/analysis/pics/Cytobank_43324_NG_markers.png}
    \end{center}
    \caption{
        Marker intensities of differentially abundant subpopulations in the Nanog-GFP time course, detected at a spatial FDR of 5\%.
        Each plot corresponds to a marker while each point represents a hypersphere, coloured according to its median intensity for the corresponding marker.
    }
\end{figure}

\begin{figure}[p]
    \begin{center}
    \includegraphics[width=0.6\textwidth]{../real/analysis/pics/Cytobank_43324_NN_logFC.png}
    \end{center}
    \caption{
        Differentially abundant subpopulations in the Nanog-Neo time course, detected at a spatial FDR of 5\%.
        Each point is a hypersphere and is coloured according to its log-fold change in abundance over time.
        Grey points are hyperspheres with significant but non-linear changes in abundance.
    }
\end{figure}

\begin{figure}[p]
    \begin{center}
    \bigfigopt{../real/analysis/pics/Cytobank_43324_NN_markers.png}
    \end{center}
    \caption{
        Marker intensities of differentially abundant subpopulations in the Nanog-Neo time course, detected at a spatial FDR of 5\%.
        Each plot corresponds to a marker while each point represents a hypersphere, coloured according to its median intensity for the corresponding marker.
    }
\end{figure}

\begin{figure}[bt]
    \begin{center}
        \includegraphics[width=\textwidth]{mef_timing.pdf}
    \end{center}
    \caption{
        Changes in cell abundance after transgene induction with doxycycline (left) or after doxycyline withdrawal (right) in the Oct4-GFP time course.
        Each point in the $t$-SNE plot represents a differential hypersphere, coloured based on the log-fold change during induction (day 0 to 1) or withdrawal (day 16 to 17).
        A prior of 3 was added to each count to stabilize the log-fold changes.
    }
    \label{fig:importanttime}
\end{figure}


\begin{figure}[p]
    \begin{center}
        \includegraphics[width=0.6\textwidth]{../real/analysis/pics/novel_timecourse.pdf}
    \end{center}
    \caption{
        Cell abundance for the novel SC4-like subpopulation with active STAT3, AMPK and PLK1 signalling, as a function of time in the Oct4-GFP reprogramming time course.
        Abundances are shown as a percentage of the total number of cells in each sample.
        Each line represents the abundance for each hypersphere representing the subpopulation, while the black line is the average across all hyperspheres.
    }
    \label{fig:nonlinear}
\end{figure}

\begin{figure}[p]
    \begin{center}
        \includegraphics[width=\textwidth,trim=10mm 40mm 0mm 0mm,clip]{pics/Figure_BMMC.pdf}
    \end{center}
    \caption{Differentially abundant subpopulations in the BMMC data set upon treatment with IL-10, detected at a spatial FDR of 5\% and visualized with a $t$-SNE plot.
            Each point represents a hypersphere and is coloured according to its average log-fold change in abundance upon treatment.
            Hyperspheres were annotated into subpopulations based on expression of lineage-specific markers and signalling molecules.
    }
    \label{fig:bmmc}
\end{figure}

\begin{figure}[p]
    \begin{center}
    \bigfigopt{../real/secondary/pics/IL10_markers.png}
    \end{center}
    \caption{
        Marker intensities of differentially abundant subpopulations upon IL-10 stimulation in the BMMC data set, detected at a spatial FDR of 5\%.
        Each plot corresponds to a marker while each point represents a hypersphere, coloured according to its median intensity for the corresponding marker.
        The colour range for each marker was bounded at the 1\textsuperscript{st} and 99\textsuperscript{th} percentiles of intensities across all cells.
    }
    \label{fig:bmmc_markers}
\end{figure}

\begin{figure}[p]
\begin{center}
\includegraphics[width=0.49\textwidth]{../simulations/cluster_setup.pdf}
\includegraphics[width=0.49\textwidth]{../simulations/plot_cluster.pdf}
\end{center}
\caption{Relative performance of cluster-based DA analyses in simulated data.
    Left: a PCA plot to illustrate the simulation design, which contains a non-DA bulk of cells (grey) along with two small, adjacent DA subpopulations exclusive to the first (blue) or second conditions (red).
    Right: detection frequencies for each of the two DA subpopulations across 20 simulation iterations.
    DA analyses were performed using CITRUS, a custom approach with varying numbers of clusters $k$ or with hyperspheres.
}
\label{fig:clustersim}
\end{figure}

\begin{figure}[p]
\begin{center}
\includegraphics[width=0.8\textwidth]{../real/clustering/Cytobank_43324_4FI_citrus.png}
\end{center}
\caption{Detected clusters from a differential abundance analysis of the Oct4-GFP time course with CITRUS, mapped onto the $t$-SNE plot of DA hyperspheres.
Each point represents the centre of a DA cluster that was detected at a FDR of 5\%.
DA hyperspheres are shown as a grey outline for comparison.
}
\label{fig:clusterreal}
\end{figure}


\end{document}
