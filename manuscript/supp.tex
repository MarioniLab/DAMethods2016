\documentclass{article}
\usepackage{graphicx}
\usepackage[margin=2.5cm]{geometry}
\usepackage[labelfont=bf]{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{xr}
\usepackage{xcite}
\usepackage{amsmath}
\usepackage[euler]{textgreek}

\renewcommand{\textfraction}{1.0}
\renewcommand{\floatpagefraction}{.9}
\newcommand\revised[1]{\textcolor{red}{#1}}
\renewcommand{\topfraction}{0.9}    % max fraction of floats at top
\renewcommand{\bottomfraction}{0.8} % max fraction of floats at bottom
\renewcommand{\textfraction}{0.07}  % allow minimal text w. figs

\makeatletter 
\renewcommand{\fnum@figure}{Supplementary \figurename~\thefigure}
\renewcommand{\fnum@table}{Supplementary \tablename~\thetable}
\makeatother

%\renewcommand{\thefigure}{S\@arabic\c@figure} 
%\renewcommand{\thetable}{S\@arabic\c@table} 

\externaldocument{description}
\externalcitedocument{description}

\usepackage{url}
\urlstyle{same}

\begin{document}

\begin{titlepage}
\vspace*{3cm}
\begin{center}

{\LARGE
Testing for differential abundance in mass cytometry data
\par}

\vspace{0.75cm}

{\Large 
    \textsc{Supplementary Materials}
\par
}
\vspace{0.75cm}

\large
by

\vspace{0.75cm}
Aaron T. L. Lun$^{1}$, Arianne C. Richard$^{1,2}$ and John C. Marioni$^{1,3,4}$

\vspace{1cm}
\begin{minipage}{0.9\textwidth}
\begin{flushleft} 
$^1$Cancer Research UK Cambridge Institute, University of Cambridge, Li Ka Shing Centre, Robinson Way, Cambridge CB2 0RE, United Kingdom \\[6pt]
$^2$Cambridge Institute for Medical Research,  University of Cambridge, Wellcome Trust/MRC Building, Hills Road, Cambridge CB2 0XY, United Kingdom \\[6pt]
$^3$EMBL European Bioinformatics Institute, Wellcome Genome Campus, Hinxton, Cambridge CB10 1SD, United Kingdom \\[6pt]
$^4$Wellcome Trust Sanger Institute, Wellcome Genome Campus, Hinxton, Cambridge CB10 1SA, United Kingdom \\[6pt]
\end{flushleft}
\end{minipage}

\vspace{1.5cm}
{\large \today{}}

\vspace*{\fill}
\end{center}
\end{titlepage}

\section{Defining the hypersphere radius for cell counting}

\subsection{Formulating an expression for the radius}
Consider a subpopulation where the intensities of each marker are distributed around some mean intensity with standard deviation $r_0$.
%The center of the subpopulation is defined by the set of expected intensities.
The expectation of the squared Euclidean distance between any two cells in this subpopulation is $2r_0^2M$.
Now, construct hyperspheres centred at each of the cells in the subpopulation.
Setting the hypersphere radius as $r=r_0\sqrt{2M}$ ensures that most hyperspheres will contain a large proportion of cells from the same subpopulation.
Indeed, a simple simulation with Normally-distributed intensities for 30 markers indicates that the majority of hyperspheres contain more than 50\% of the subpopulation.

% To wit, E[(Xa - Xb)^2] = E(Xa^2) + E(Xb^2) - 2E(XaXb)
%                        = r0^2 + u^2 + r0^2 + u^2 - 2u^2
%                        = 2r0^2
% And then sum this over all markers. We can demonstrate with some code:
%   a <- matrix(runif(30000, 0, r), ncol=30)
%   out <- as.matrix(dist(a))
%   sqrt(var(a[,1])) * sqrt(30) * sqrt(2)
%   mean(out)
% The same works with normally distributed intensities:
%   r <- 1
%   a <- matrix(rnorm(30000, 1, r), ncol=30)
%   out <- as.matrix(dist(a))
%   sum(colSums(out <= r*sqrt(2*ncol(a))) >= nrow(a)/2)

We further assume that the expression of a marker will differ by up to 10-fold across functionally similar cells, due to technical noise or biological variability within the same subpopulation.
\revised{(Expression profiles for more variable markers like CD25 may consist of multiple internal subpopulations.)}
For typical analyses involving transformed intensities on the $\log_{10}$-scale, an interval of length 1 will span an order of magnitude in marker expression.
When centred around the mean, this interval should contain most of the intensity distribution for a single subpopulation.
According to Chebyshev's inequality, the interval defined by a distance of $r_0\sqrt{2}$ from the mean will contain at least 50\% of the values from an arbitrary distribution.
We equate these interval lengths to obtain a rough estimate for the standard deviation of the intensities, i.e., $2r_0\sqrt{2} = 1$.
Applying this to the radius yields $r=0.5\sqrt{M}$, meaning that each hypersphere will be able to include cells with 10-fold differences in expression for one or more markers.
Thus, each hypersphere will contain enough counts for further analysis, even when the intensities vary across an order of magnitude.

% This always does my head in a bit; Chebyshev is conservative in that the root-2 is a larger than it should be for most distributions.
% However, this means that the estimate of r_0 is lower than what it should be for most distributions.
% All this does is provide us with a bit more precision at the cost of fewer counts, so I guess that's fine.

% (A lower proportion of cells in the subpopulation will be counted if the hypersphere is not centred at the subpopulation centre.
% This will result in reduced power to detect differential abundance for that particular hypersphere.
% However, as long as there is one hypersphere that is centred at the subpopulation centre, then the potential to reject the null hypthesis for the corresponding subpopulation will not be lost.
% Recall that each hypersphere is centred at an observed cell, so any subpopulation in the data set with a reasonable number of cells ($>100$) is likely to be captured by an appropriate hypersphere near the subpopulation centre.)

We verify the choice of formulation of $r$ by examining the distance between each cell $x$ and its neighbours in the first sample from each time course.
The nearest neighbours of $x$ are identified in the full $M$-dimensional space and represent other cells in the same subspace as $x$.
As the number of markers increases, the distance to the neighbours increases at a rate that is roughly consistent with the square root function (Supplementary Figure~\ref{fig:radius}).
This justifies the use of the relation $r\propto\sqrt{M}$.
With increasing $M$, the radius will also increase such that a hypersphere centred at $x$ will still contain the neighbours of $x$, i.e., sensitivity of counting for cells in the same subspace is preserved.
In contrast, cells in other subspaces will cease to be assigned to the hypersphere as they become separated from $x$ on other markers, i.e., specificity of counting is improved.
 
% A consequence of setting $r$ like this is that, in any one dimension, it is possible to count cells that are more than $r_0$ from the centre of the hypersphere.
% This is inevitable in multi-dimensional space, where you need to account for scatter in dimensions other than the one of immediate interest.
% Setting $r$ any smaller would fail to count the majority of cells derived from the same subpopulation.
% Indeed, a more straightforward definition of each subspace would be to only count cells that are less than $r_0$ away from the centre in every dimension.
% This defines the subspace as an $M$-dimensional box with sides of length $2r_0$.
% However, the use of boxes is not practical as the probability of cells falling into a box will decrease with increasing $M$. 
% This results in near-zero counts (or counts of 1, for boxes centred on an existing cell).
% Hyperspheres are more suitable at large $M$, which motivates their use above.
  
For the Oct4-GFP and Nanog-Neo time courses, the radius is consistent with the median distances from each cell to its 10\textsuperscript{th} nearest neighbour.
This means that around 10 cells from the subspace will be assigned to each hypersphere when $r_0\sqrt{2}=0.5$.
Combined with counts from the other samples in the time course, this provides enough information to reliably detect DA hyperspheres.
In contrast, the radius in the Nanog-GFP time course is consistent with the distance of each cell to its 1\textsuperscript{st} nearest neighbour.
This is because the first sample in the Nanog-GFP time course contains fewer than 3000 cells, while samples in the other time courses contain over 10000 cells.
The sparsity of cells in this sample results in fewer neighbours at any given distance.
For experiments with few cells across all samples, a larger $r_0$ may be required to obtain sufficient counts for further analysis.
(Note that we keep $r_0\sqrt{2}=0.5$ for Nanog-GFP, as the other samples in the time course contain 5-10-fold more cells.
Thus, counts will be large enough to detect differences later on.)

\subsection{Additional remarks on radius choice}
\label{sec:additionalradius}

\begin{color}{red}
The suitability of the default setting of $r_0\sqrt{2}=0.5$ can be assessed for each data set by examining the distance from each cell to its nearest neighbours.
Supplementary Figure~\ref{fig:nvd} shows these distances for each cell in the Oct4-GFP reprogramming time course.
Here, the default radius corresponds to the median of the distances from each cell to its 15\textsuperscript{th} nearest neighbour.
This means that half of all hyperspheres will contain more than 15 cells, which is large enough for downstream hypothesis testing.
In contrast, a smaller value of the radius (e.g., $r_0\sqrt{2}=0.4$) would not even include the nearest neighbour for each cell on average.
This would lead to a count of 1 for most hyperspheres, which is not sufficient for further statistical analysis.

A consequence of working in high-dimensional space is that small changes to the radius will result in large changes to the counts.
For example, in an experiment consisting of 30 markers, increasing the radius by 10\% will increase the hypersphere volume (and the potential number of counted cells) by over 17-fold. 
In this respect, the analysis is quite sensitive to the choice of radius.
However, a more relevant assessment of sensitivity is that with respect to changes in the counts per hypersphere.
We repeated the DA analyses using radii values that approximately halved or doubled the median number of counted cells in each hypersphere relative to the default (Supplementary Figure~\ref{fig:nvd}).
We observed that most of the DA hyperspheres from the default analysis were still detected in the repeated analyses (Supplementary Table~\ref{tab:param}).
Over 60\% of the DA hyperspheres were recovered with a smaller radius, while using a larger radius detected almost all of them (and an extra 35\%).
These differences are expected because larger hyperspheres have greater counts and more evidence to reject the null hypothesis, albeit at the cost of spatial resolution.
Nonetheless, the general similarity in the results indicates that the analysis is robust to substantial changes in the size of the counts.
\end{color}

\revised{An obvious question is whether an ``optimal'' value of $r_0$ can be obtained for any given data set.}
This \revised{value} depends on the variability of marker intensities in \revised{each} subpopulation.
For example, setting $r_0$ to the standard deviation of the marker intensities \textit{for a single subpopulation} allows more cells associated with that subpopulation to be counted into the hypersphere, while reducing the chance of counting cells from adjacent subpopulations.
However, the standard deviation of each subpopulation is not easy to estimate empirically, as we do not know the true number of subpopulations in the $M$-dimensional space.
Indeed, a single optimal value may not exist, e.g., if different subpopulations or markers have different standard deviations.

\revised{A suboptimal} choice of $r_0$ is not a critical concern for a DA analysis.
A value of $r_0$ that is too small will reduce power to detect a DA subspace as the counts are too low.
A value that is too large will also reduce power \revised{through the loss of spatial resolution -- specifically,} cells from non-\revised{differential} subspaces \revised{will be} included in the \revised{counts} for \revised{a} hypersphere \revised{in a differential subspace, reducing the log-fold change in abundance.}
In both cases, power is affected rather than error rate control.
While loss of power is undesirable, the analysis will still be valid as any discoveries (that are made in spite of the diminished power) can be trusted.
We note that ``contamination'' of a non-DA hypersphere with cells from \revised{differential} subspaces is also possible for large $r_0$, which could lead to the detection of the former as a false positive.
However, the effect of this contamination is mitigated by the fact that the position of each hypersphere is defined using the median intensities.
Any contamination that substantively changes the counts for a hypersphere will also shift its position towards the differential subspace.
Thus, a contaminated hypersphere that is erroneously classified as \revised{DA} will be assigned to or near the differential subspace (based on its position), rather than a non-differential subspace (based on its centre).
This reduces the detection rate of false positives in the non-differential subspace.

\begin{color}{red}
To demonstrate this effect, we set up a simulation where we examined the effect of increasing the radius on the position of each hypersphere (Supplementary Figure~\ref{fig:radius_schematic}).
When larger radii were used, hyperspheres centred on cells from a non-DA subpopulation had larger log-fold changes in abundance (Supplementary Figure~\ref{fig:radius_position}).
This is consistent with increasing contamination from cells in the neighbouring DA subpopulation.
However, the positions of the affected hyperspheres were also shifted towards the DA subpopulation.
In most simulation scenarios, this shift was sufficiently large that the hyperspheres with large log-fold changes ($>1$) were not distinguishable from hyperspheres centred on cells from the DA subpopulation itself.
The only exception occurs when the size of the DA subpopulation was reduced, such that some of the non-DA hyperspheres have large log-fold changes but lie outside the subpopulation boundaries.
Even in this case, a substantial shift in position towards the DA subpopulation was observed, mitigating the effect of misinterpreting these hyperspheres as part of a differential subspace.
Using very large hypersphere radii also results in a decrease in the log-fold changes for hyperspheres centred on cells from the DA subpopulation.
This is consistent with loss of power when cells from non-DA subspaces are included in the counts.
\end{color}

\begin{color}{red}
\section{Strategies for dealing with intensity shifts}

\subsection{Overview}
In barcoded experiments, technical effects causing shifts in marker intensity between samples are avoided \cite{zunder2015palladium}.
This is due to use of multiplexed staining and cytometry, which ensures that any fluctuations in the experimental procedures (e.g., staining efficiency or concentration) affect all samples equally.
While barcoding is becoming more common \cite{gaudilliere2014delayed,gaudilliere2015implementing}, its application is not always possible.
For example, if the number of samples is greater than the number of available barcodes, samples will need to be split into batches for separate barcoding and analysis.
This is likely to introduce intensity shifts between batches, where similar cells in different batches have different observed marker intensities due to technical factors.
For hypersphere counting, these shifts are problematic as cells from the same subpopulation may no longer be counted into the same hypersphere across samples.
This has some detrimental consequences for the downstream statistical analysis (see below).
Here, we present some strategies for handling intensity shifts in several scenarios.

\subsection{Intensity shifts between separately barcoded batches}

\subsubsection{Outline of the normalization method}
Consider an experimental design containing multiple batches of separately barcoded samples, such that intensity shifts are present between but not within batches.
Intensities can be normalized between batches by assuming that the pooled intensity distribution across samples in each batch should be the same between batches (i.e., if intensity shifting were not present).
This is usually reasonable in experimental designs where each batch contains samples for the same or similar set of conditions.
The aim of the normalization procedure is to coerce the pooled intensity distributions for all batches to the same reference distribution, thereby removing any intensity shifts or changes in the shape of the intensity distribution between batches.

\subsubsection{Constructing the quantile function}
The first step is to construct a quantile function from the weighted distribution of intensities:
\begin{enumerate}
    \item Let the number of samples for condition $g$ in batch $b$ be denoted as $N_{bg}$.
        The average sample size $\bar{N}_g$ for condition $g$ is computed by taking the mean of $N_{bg}$ across all batches.
    \item Consider sample $j$ of condition $g$ in batch $b$, which contains $T_{jgb}$ cells.
        Each cell is assigned a weight of 
        \[
            w_{jgb} = \frac{\bar{N}_g}{N_{bg}T_{jgb}} \;.
        \]
    \item Cells are pooled across all samples within each batch.
        A weighted distribution of intensities is generated for each marker $m$, based on the intensities and weights for all cells in the batch.
        This is used to generate a batch- and marker-specific quantile function $q_{mb}(.)$ for the intensity distribution.
\end{enumerate}
The weights ensure that the contribution of each condition is the same across batches, to accommodate experimental designs where $N_{bg}$ or $T_{jgb}$ differs between batches.
Otherwise, the pooled intensity distributions may not be the same across batches if some samples or conditions contribute more to one batch than to others.
Note that any conditions without samples in some batches are ignored during construction of $q_{mb}(.)$.
No amount of weighting can overcome the absence of information about these conditions in the affected batches.

\subsubsection{Performing quantile normalization across batches}
The second step is to apply the quantile functions for normalization of the marker intensities:
\begin{enumerate}
    \item Construct an average quantile function $q_m(.)$ for each marker by taking the average of $q_{mb}(.)$ across all batches.
        This represents the reference intensity distribution to which all others should be coerced.
    \item For each batch, compute the reference quantile $Q'_{ijgb}$ corresponding to the intensity $Q_{ijgb}$ for each cell $i$ used to construct $q_{mb}(.)$.
        This is done by taking the empirical cumulative probability at $i$ (i.e., the proportion of $w_{jgb}$ assigned to cells with intensities below $Q_{ijgb}$) and using that as input to $q_m(.)$.
    \item Define an batch-specific adjustment function $a_{mb}(.)$ that takes $Q_{ijgb}$ as input and returns $Q'_{ijgb}$.
    \item Apply $a_{mb}(.)$ to the intensities for marker $m$ in all samples of batch $b$.
        This yields normalized intensity values where the weighted distribution for each batch is coerced to the reference. 
\end{enumerate}
Of particular note is that, even though some conditions may not contribute to the construction of $q_{mb}(.)$ in any batch, the adjustment function $a_{mb}(.)$ can still be applied to the intensities of samples in those conditions.
This allows normalization of intensities for samples in conditions that are not present in all batches.

There are some important caveats to note with this method.
Firstly, it requires a semi-balanced design with samples from at least one group in each batch.
Ideally, at least one sample from each group would be present in each batch, such that all samples would be able to contribute to the weighed distribution for each batch.
Secondly, it assumes that the only systematic difference between batches is caused by technical factors -- this is discussed in more detail in the example below.
Finally, the application of $a_{mb}(.)$ to samples not involved in constructing $q_{mb}(.)$ assumes that the intensities in those samples are within the range of all $Q_{ijgb}$.
Intensities beyond this range require extrapolation, which is not easily achieved by quantile normalization.

\subsubsection{Performing range normalization across batches}
Instead of performing quantile normalization, a simpler approach is to scale the marker intensities so that the maximium and minimum of the weighted distribution is the same for each batch.
We compute the reference minimum and maximum as $q_m(0.01)$ and $q_m(0.99)$, i.e., the 1\textsuperscript{st} and 99\textsuperscript{th} percentiles of the reference distribution for marker $m$.
(We use percentiles rather than taking the actual extremes, in order to protect against outliers.)
For each batch $b$, a linear scaling function $a'_{mb}(.)$ is defined that converts $q_{mb}(0.01)$ and $q_{mb}(0.99)$ to the reference minimum and maximum, respectively.
This scaling function is then applied to the marker intensities for all samples in batch $b$ to obtain normalized intensities.
Unlike quantile normalization, range-based normalization approach avoids assuming that all differences between batches are technical.
Biological differences between batches are accommodated, provided that they do not alter the ranges of the pooled distribution of intensities.
However, range-based normalization also assumes that the technical differences between batches can be fully described in terms of linear shifts or scaling of the distribution.
More complex non-linear effects are not supported, whereas they would be handled properly with quantile normalization.

\subsubsection{Application of the normalization methods to real data}
As a demonstration, we applied our normalization procedure to the BMMC data set generated by Levine \textit{et al.} \cite{levine2015datadriven}.
This data set consists of drug-treated and untreated samples from multiple individuals, where barcoding was performed within but not between individuals, i.e., each individual represents a batch.
Before normalization, we observed large differences in the intensity distribution of some markers between corresponding samples from different individuals (Supplementary Figure~\ref{fig:quantile}).
Part or all of these differences are likely caused by the technical effects of separate staining and cytometry between individuals.
Shifts in the maximum intensity were successfully eliminated upon range-based normalization.
Application of quantile normalization also removed changes in the shape of the intensity distribution between individuals.

It is worth noting, however, that this experimental design is not ideal for our normalization methods.
This is because the technical differences between batches are confounded by genuine biological differences between individuals.
Subsequently, normalization to remove the former may also remove or distort the biological effects.
A more suitable design would contain samples from multiple individuals in each batch, such that one could assume that there were no biological differences between the average of individuals within each batch.
To mitigate any distortions of the underlying biology in this data set, we only apply range-based normalization as this makes weaker assumptions regarding the nature of the inter-batch differences.
Nonetheless, the DA analysis following our normalization still yields sensible results, as described in Section~???.

\subsection{Intensity shifts between non-barcoded samples}
A more difficult situation is that of a data set containing samples with no barcoding at all.
For an intensity shift between two samples in different conditions, it is impossible to determine whether the shift represents a technical or biological effect.
Subsequently, normalizing the location of the intensity distributions risks discarding interesting biology.
Instead, we use a different approach where the radius of the hyperspheres is increased by the size of the shift.
Cells from the same subpopulation in different samples are more likely to be counted into the same expanded hypersphere, even after shifting of intensities between samples.

To calculate the magnitude of the intensity shift due to technical effects, we compute the mean intensity of each marker in each sample.
This yields a vector of means $\mathbf{u}_m$ for each marker $m$.
We fit a linear model to $\mathbf{u}_m$ for each $m$, using an appropriate design matrix that describes the experimental setup.
The sample variance of the fitted model provides an estimate of the average squared shift between replicate samples.
As this component of the shift occurs between replicates, it is more likely to be technical in origin.
We average the variance estimates across all markers to obtain $s^2$, the extra variance introduced by the shifting process.
Stochastic intensity shifts between samples increase the squared Euclidean distances between cells (from the same subpopulation but in different samples) by $2s^2$ for each marker.
Thus, the new hypersphere radius becomes $\sqrt{2M(r_0^2 + s^2)}$ in order to be able to routinely assign such cells to the same hypersphere.

% This is a logical extension of the previous statements. If you consider that cells 'a' and 'b' in different samples get an extra random 'Ya' and 'Yb':
%   E[(Xa + Ya - Xb - Yb)^2] = 2(r0^2 + s^2)
% Or for some code:
%   sqrt(mean((rnorm(10000, sd=0.5/sqrt(2)) + rnorm(10000, sd=0.25)
%            - rnorm(10000, sd=0.5/sqrt(2)) - rnorm(10000, sd=0.25))^2))
%   sqrt(0.25^2 * 2 + 0.5^2)

We tested this approach with simulations based on the reprogramming time courses.
Simulated data were generated in the same manner as described for assessing type I error control, using only the first sampling scheme for simplicity.
Then, for each sample and for each marker, we sampled an intensity shift from a Normal distribution with a mean of zero and a standard deviation ranging from 0 to 0.3.
This shift value was added to the marker intensities for all cells in that sample.
We counted cells into hyperspheres with and without expansion of the radius, performed the DA analysis and computed the observed type I error rate.
For small shifts, the type I error rate was controlled with both the default and expanded radii (Supplementary Figure~\ref{fig:shift_sim}).
This is due to an increase in the negative binomial dispersion to account for greater variability in counts between replicates, when cells from the same subpopulation are shifted in or out of the hypersphere between samples. 
For larger shifts, increases to the dispersion were not sufficient to control the type I error rate with the default radius.
This is because the movement of entire subpopulations in or out of hyperspheres yields cell counts that are not accurately modelled with the NB distribution.
In contrast, the use of an expanded radius mitigates the loss of error control.
This reduces the effect of the shifts on the cell counts for each hypersphere, which allows for smaller dispersions and ameliorates the inaccuracy of the NB model.

There are some important caveats with this expansion approach, some of which are mentioned below.
Firstly, we assume that the variance of the shifting process is the same for different markers.
This may not be true if some antibodies are more susceptible than others to technical effects.
Secondly, we assume that shifts in location between replicates are wholly technical.
This is unlikely to be true, given that biological variability will be present between, e.g., replicate mice or patients.
Thirdly, as observed above, accurate control of the type I error rate is not actually achieved upon expansion.
The improvement is only a mitigation of the more severe loss of control when the default radius is used.
Finally, expansion of the hypersphere radius leads to loss of spatial resolution, which can reduce power to detect changes in abundance.
While these shortcomings are significant, the lack of barcoding in the data itself leaves few analytical options for rigorous quantitation across samples.
In these cases, radius expansion seems to be the best strategy to handle intensity shifts.
\end{color}

\begin{color}{red}
\section{A brief description of edgeR's statistical framework}
The following section largely paraphrases Lun \textit{et al.} \cite{lun2016delicious}, with some minor adjustments.
Consider a hypersphere $h$ with count $y_{hj}$ in each sample $j$.
In edgeR, the count in each sample is modelled with a quasi-negative binomial distribution with mean $\mu_{hj}$.
(We will discuss the dispersion parameters of this distribution later.)
Using a log-link generalized linear model with $K$ coefficients \cite{mccarthy2012differential}, the mean is represented as
\[
    \log \mu_{hj} = \sum_{k=1}^K x_{jk} \beta_{hk} + o_{hj} \;,
\]
where $\beta_{hk}$ is the hypersphere-specific value of coefficient $k$, $x_{jk}$ is the sample-specific predictor for $k$, and $o_{hj}$ is the hypersphere- and sample-specific offset.
For simple one-way layouts, each coefficient represents the (log-transformed) average proportion of cells within a group, while each predictor specifies the group to which each sample belongs.
More complex designs can also be used where coefficients represent blocking factors or real-valued covariates.
The offset for each sample is defined as the log-transformed total number of cells, and ensures that differences in the numbers of cells between groups do not cause differences in $\beta_{hk}$.

With the quasi-likelihood methods in edgeR, the mean-variance relationship of each count is modelled as
\[
    \mbox{var}(y_{hj}) = \sigma^2_{h} (\mu_{hj} + \mu_{hj}^2 \phi_h) \;,
\]
where $\sigma^2_h$ is the shrunken quasi-likelihood dispersion and $\phi_h$ is the trended negative binomial dispersion.
Any increase in the observed variance of the counts will be modelled by an increase in these two dispersions.
The NB dispersion for each hypersphere is set to the fitted value of a mean-dispersion trend \cite{mccarthy2012differential}, i.e., $\phi_h = \phi(\mu_h)$ where $\phi$ is the trend function and $\mu_h$ is the average count across all samples for each hypersphere $h$.
This allows empirical mean-variance relationships to be accurately modelled.
The raw QL dispersion for each hypersphere is estimated from the deviance of the fitted GLM.
A separate mean-dependent trend is fitted to the raw QL dispersions against $\mu_h$ for all hyperspheres, and empirical Bayes shrinkage is performed to squeeze the raw estimates towards this trend \cite{lund2012detecting}.
The resulting values are referred to as the shrunken QL dispersions $\sigma^2_h$, and allow hypersphere-specific variability to be modelled in the presence of limited replication.

Hypothesis testing is performed by formulating null hypotheses in terms of the various $\beta_{hk}$.
For example, in a one-way layout with groups $k\in\{1,2\}$, one could test for differential abundance between groups by testing the null hypothesis $\beta_{h1}=\beta_{h2}$ for each hypersphere.
A similar approach can be used for more complex designs -- for example, if a spline was fitted to the abundances with respect to time, the corresponding coefficients can be set to zero under the null to test for any time effect.
The $p$-value for each hypersphere is calculated with the QL F-test, which accounts for the uncertainty in estimating the shrunken QL dispersions. 
\end{color}

\section{Handling composition effects in the counts}
Composition effects \revised{refer to indirect changes to the proportion of cells assigned to a subspace, caused by changes to the total number of cells between conditions.
For example, consider a situation where one subspace experiences a large increase in cell abundance between conditions, while the abundances in all other subspaces remain the same.
The increase in the former drives an increase in the total number of cells in the affected condition.
This results in a decrease in the \textit{proportion} of cells that are assigned to the other subspaces, leading to differences being observed in hyperspheres that do not directly exhibit a change in abundance.}
From a mathematical perspective, this is not incorrect as the proportions \textit{do} change when the total counts are altered.
Nonetheless, detection of such subspaces may not be biologically relevant, so composition effects should ideally be removed prior to further analysis.
Unfortunately, conventional strategies for normalizing these effects (from RNA-seq data analysis \cite{robinson2010scaling}) are not applicable here.
If these methods were applied to the hypersphere counts, they would assume that most hyperspheres are not differentially abundant.
This is unlikely to be true in many settings, e.g., due to large-scale changes upon stimulation or activation.

Rather, alternative strategies are required to mitigate composition effects.
The simplest approach is to gate out any high-abundance differential subpopulations.
The total count can then be calculated from the remaining cells, which avoids introducing composition effects from the differential subpopulation.
Identification of problematic subpopulations can be done before the DA analysis based on existing knowledge, or afterwards based on the top DA hyperspheres.
For example, \revised{consider a mixed population of T and B cells.
    If these cells were analyzed together, a large increase in the number of B cells in one condition would result in a concomitant decrease in the \textit{proportion} of T cells -- even if the number of T cells did not change between conditions.
    This result would suggest that the T cell population decreases in abundance, which might be misleading.} 
Instead, one can gate on CD3 or CD19 to isolate T or B cells, respectively, \revised{and then analyze each of the gated populations separately.}
This avoids detecting \revised{indirect} changes in T-cell subpopulations due to changes in B-cell abundance, and vice versa.
Note that this is only necessary for DA subpopulations with many cells, as changes in small subpopulations will not have a substantial effect on the total count.

Another approach is to test for differential abundance against a minimum log-fold change threshold.
This avoids detecting small changes in abundance caused by composition effects.
Such changes may be statistically significant but are unlikely to be biologically relevant \cite{mccarthy2009treat}, and are subsequently ignored.
While this is a more general approach than gating, it only protects against small composition effects -- large changes in the dominant subpopulation may induce large changes in abundance across all hyperspheres.

\section{Controlling the spatial FDR}

\begin{color}{red}
\subsection{A detailed explanation of the spatial FDR}
Let us split the $M$-dimensional space into arbitrarily small non-overlapping partitions of similar volume.
For example, these partitions might be pixels in two-dimensional space, or voxels in three-dimensional space.
Each hypersphere is assigned to the partition containing its median-based position.
The outcome of the test for differential abundance for the hypersphere is used as a proxy for the outcome of its assigned partition (Supplementary Figure~\ref{fig:fdrdemo}).
In the simplest scenario, a partition containing only one hypersphere will be represented by that hypersphere.
If multiple hyperspheres are assigned to a partition, a single hypersphere is randomly selected as the representative.
We define the FDR across the expected set of representative hyperspheres (defined as the set of all hyperspheres, where each hypersphere is weighted by the probability of being sampled from its partition) as the spatial FDR.
By controlling this value below a specified threshold, we effectively control the FDR across partitions, and thus, the FDR across the volume of those partitions.

In practice, we do not need to explicitly define partitions in order to control the spatial FDR.
Rather, the probability of sampling a representative hypersphere from a partition is inversely proportional to the density of assigned hyperspheres in that partition.
Given that our definition above considers small partitions of similar size, the local density of each hypersphere can approximate the density of the partition to which it was assigned.
Thus, instead of directly allocating hyperspheres into partitions and computing the partition density (which would involve arbitrary definitions of the shape, size and arrangement of partitions), we compute the local density of each hypersphere by applying a kernel density estimator to the positions of all tested hyperspheres.
The weight of each hypersphere is defined as the reciprocal of its local density.
This approximates the relative probability (scaled by some constant, which can be ignored) of sampling that hypersphere as a representative of its partition.
The Benjamini-Hochberg method is applied to the hypersphere $p$-values with the associated weights.
This controls the FDR across the expected set of representative hyperspheres.

% The bandwidth parameter is analogous to the size of the partitions.
% In general, it must be small enough so that the the hyperspheres within each hypothetical partition are well-correlated.
% Otherwise, the FDR over the expected set would not approximate the expected FDR over all possible sets.
% One might argue that larger partitions are more relevant to interpretation, given that people generally won't pore over every subspace.
% However, if you increase the bandwidth, you'll eventually end up with all weights being equal, i.e., naive BH.
% The bandwidth here is chosen based on nearest-neighbour considerations, so it will shrink with an increasing number of cells.
% In other words, you'll get a more precise estimate of the density, or a more precise definition of the volume around the DA hyperspheres.

As described in Figure~\ref{fig:fdrexample}, the spatial FDR can be roughly interpreted as the proportion of the volume occupied by DA hyperspheres that corresponds to false positives.
This is based on considering partitions of similar size to the hyperspheres, such that the total volume of the partitions, and the proportion of which is false positive, is similar to that of the hyperspheres.
(The corresponding assumption in our control procedure would be that the kernel bandwidth is similar to the hypersphere radius.
However, the results are quite robust to the choice of bandwidth -- see Supplementary Table~\ref{tab:param} -- so we will ignore this subtlety.)
While a definition of the spatial FDR based on the hypersphere volumes is intuitive, it is difficult to implement.
Computing the total volume of (overlapping) hyperspheres in high-dimensional space is not straightforward.
It is also unclear how to define the ``false positive volume'' in the presence of overlaps between a false and true positive hypersphere.
Our definition of the spatial FDR, while less intuitive, is easier to control.
\end{color}

\subsection{Applying the weighted Benjamini-Hochberg procedure}
Let each null hypothesis $i$ be associated with a $p$-value $p_{(i)}$ and a frequency weight $f_{(i)}$.
Assume that there are $n$ null hypotheses, ordered such that $p_{(1)} < p_{(2)} < ... < p_{(n)}$.
To control the FDR at some threshold $\alpha$, a weighted BH procedure is applied \cite{benjamini1997multiple} to reject any null hypothesis where the $p$-value is less than
\[
    T = \max\left\{ p_{(i)} : p_{(i)} \le \alpha \frac{\sum_{l=1}^{i} f_{(l)}}{\sum_{l=1}^{n} f_{(l)}} \right\}  \;.
\]
%Intuitively, this can be understood as treating the weighted hypothesis $i$ as a collection of $f_{(i)}$ unweighted hypotheses, and applying the standard BH method to the total set of unweighted hypotheses across all $i$.
%(This reasoning is still applicable for non-integer $f_i$, as such values can be scaled up to yield values that are arbitrarily close to integers.
%The calculation of $T$ is unaffected as any scaling will cancel out.
%Moreover, while the unweighted hypotheses in each collection are completely dependent, this should not compromise FDR control as the BH method is robust to dependencies \cite{reiner2003identifying,kim2008effects}.)

To control the spatial FDR, we apply the weighted BH method to the hypersphere statistics.
Each hypersphere corresponds to one null hypothesis, while its frequency weight is defined as the reciprocal of its local density.
Here, a decision must be made regarding the choice of kernel density estimator and bandwidth.
For the latter, we compute the distance from each hypersphere position to its 50\textsuperscript{th} nearest neighbour.
The bandwidth is defined as the median of this distance across all hyperspheres.
This ensures that, on average, around 50 neighbours will be available to stably calculate the local density for each hypersphere.
We also use a tricube kernel to provide some robustness to the choice of bandwidth (Supplementary Table~\ref{tab:param}).
This gives more weight to closer neighbours while reducing the influence of cells that fall just inside the bandwidth.

\begin{color}{red}
\subsection{A discussion on the relevance of the spatial FDR}
An obvious question is, why we do not directly count cells into equally sized and spaced partitions, rather than using hyperspheres?
This would certainly simplify FDR control as the BH method could be applied directly to the $p$-values for the partitions.
However, choosing a value for the spacing parameter is not straightforward.
A value that is too small would be computationally impractical, while a value that is too large will sacrifice spatial resolution.
Our approach avoids this problem by using hyperspheres centred on cells.
This means that a hypersphere will generally be present at relevant parts of the high-dimensional space (i.e., those occupied by cells), while limiting the total number of hyperspheres to be proportional to the number of cells.

It is worth noting that we control the FDR in the expected set of representative hyperspheres, rather than the expectation of the FDR with respect to all possible sets of randomly sampled representatives.
Formally speaking, control of the FDR would refer to controlling the latter value.
The FDR across the expected set is used in our analysis because it is simpler to compute.
It also approaches the expected FDR when considering small partitions, as strong correlations betwen hyperspheres in the same partition mean that the expected set will have a similar (frequency-weighted) distribution of $p$-values as any instance of the sampled set.

In terms of interpretation, controlling the spatial FDR may not be the same as controlling the FDR across the underlying subpopulations.
For example, consider a scenario containing a DA subpopulation of large volume and a non-DA subpopulation of small volume.
Control of the spatial FDR may detect both subpopulations, as the proportion of volume taken up by the second (false positive) subpopulation is low.
However, the more obvious FDR across subpopulations would be 50\%.
In such cases, the use of the spatial FDR is justified by the potential presence of further substructure within the larger DA subpopulation.
This necessitates separate examination of each part of the first subpopulation, proportional to its volume.

% Consider an extreme case where all subpopulations reach the lower limit of size (i.e., technical + biological variability only, no substructure).
% In such cases, control of FDR over volume would be irrelevant; you would never look at part of these subpopulations, because they can't be substructure.
% However, in such cases, the spatial FDR would be roughly equivalent to controlling the FDR from the combined p-value across subpopulations.
% To illustrate, for a small subpopulation, all hyperspheres would have the same or similar p-values due to strong correlations.
% The combined p-value per subpopulation (via Simes' method) would then be pretty similar to each hypersphere p-value.
% The FDR would then be controlled across subpopulations using their combined p-values.
% If we use the weighted FDR - again, because the p-values are similar for each subpopulation, they would form a more-or-less contiguous block in the p-value ranks.
% Now, hyperspheres from each subpopulation would have the same total weight (as each subpopulation is of the same volume).
% This means that the final adjustment of the weighted individual p-values would generally be similar to that for the combined unweighted p-values.

A similar issue arises from the fact that visualization of the differential subspaces is performed in low-dimensional space to facilitate interpretation.
There is no guarantee that the FDR across, say, pixels in 2D space is equal to the spatial FDR computed in the original $M$-dimensional space.
One could overcome this problem by controlling the spatial FDR using hypersphere coordinates in low-dimensional space.
However, this seems unnecessary for data exploration.
In addition, one of the aims of the DA analysis is to simplify data visualization by showing only the subset of DA hyperspheres.
It is not clear how the FDR can be rigorously controlled across low-dimensional space if hyperspheres have been pre-selected for significance.

\section{Detailed interpretation of non-redundant hyperspheres}
While dimensionality reduction is useful for providing an overview of the data, it necessarily discards information that may actually be biologically relevant.
To assist users with more detailed interpretation of individual hyperspheres, we provide an option to prune out redundant hyperspheres.
First, hyperspheres are sorted by their $p$-value.
A hypersphere is considered non-redundant if its position is more than 1 intensity unit away (in one or more dimensions) from another non-redundant hypersphere with a lower $p$-value.
One unit represents an approximately 10-fold change in marker intensity between hyperspheres, which is large enough to warrant separate examination of those hyperspheres, though users can adjust this threshold as desired.
The non-redundancy criterion is evaluated for all hyperspheres in order of increasing $p$-value, which ensures that DA hyperspheres are reported as non-redundant rather than their neighbouring non-DA counterparts.
This procedure yields a small number of non-redundant hyperspheres that can be examined individually.
For example, over 7000 hyperspheres were detected as significantly DA in the Oct4-GFP reprogramming time course, but only 325 were considered to be non-redundant.
These can be investigated using a simple user interface like that shown in Supplementary Figure~\ref{fig:shiny}.
This provides a clear view of the median intensity of each marker and facilitates the identification of the biological subpopulation represented by each hypersphere.

\end{color}

\newcommand{\hi}{\textsuperscript{high}}
\newcommand{\lo}{\textsuperscript{low}}

\section{Annotating subpopulations in the Oct4-GFP time course}
To annotate Figure~\ref{fig:oct4} in the main text, we examined the marker intensities of each characterised subpopulation in Figure~3D of Zunder \emph{et al.} \cite{zunder2015continuous} and identified the cluster with the most similar intensities in our Supplementary Figure~\ref{fig:oct4_markers}.
The important markers for each subpopulation are listed below:
\begin{itemize}
    \item MEFs were identified as Thy1\hi{}mEF-SK4\hi{}Cd140a\hi{} and Oct4\lo{}Sox2\lo{} along with lower expression of Klf4 relative to other subpopulations.
        This was visually supported by the fact that the same cells were pS6\hi{}\textbeta-Catenin\hi{}I\textkappa{}B\textalpha\hi{} in the original figure.
    \item The OSKM non-expressing population was identified as Oct4\lo{}Sox2\lo{} with lower Klf4.
        In addition, a majority of cells exhibited lower Thy1 and Cd140a expression compared to the neighbouring MEF population.
        This was further supported by low expression of pS6 in a majority of these cells, as well as reduced \textbeta-Catenin and I\textkappa{}B\textalpha{} expression and higher p53 expression relative to MEFs.
%    \item The transition between MEFs and OSKM non-expressing cells was identified as pS6\hi{}, with moderate Thy1, mEF-SK4 and Cd140a expression.
%\textbeta-Catenin and I\textkappa{}B\textalpha{} levels were lower than in MEFs.
    \item A small subset of MEFs in Figure~\ref{fig:oct4} exhibited very high Cd140a expression.
This represents OSKM non-expressing cells that reverted to a MEF-like endpoint state after doxycycline withdrawal.
    \item The Oct4\hi{}Klf4\hi{} population was identified as it was named, supported by high expression of Sox2.
    \item The SC4-like population was identified as Klf4\hi{}Oct4\hi{}Cd73\hi{}, additionally supported by high expression of Ki67 and low expression of Sox2.
    \item Cells undergoing mesenchymal-epithelial transition were identified as EpCAM\hi{}, supported by high expression of both \textbeta-Catenin and Oct4.
    \item Ki67\lo{} reverting cells were identified as Ki67\lo{}Cd73\hi{}.
        This annotation was supported by high expression of mEF-SK4 in parts of the population.
    \item The Nanog\hi{} trajectory in the original study was identified as a Nanog-intermediate continuum from Nanog\lo{} to Nanog\hi{} cells.
        This manifests in Supplementary Figure~\ref{fig:oct4_markers} as several Nanog-intermediate subpopulations close to the Lin28\hi{} and ESC-like subpopulations.
        The annotation was supported by high expression of EpCAM and SSEA1, as well as higher pS6 expression in some parts of the trajectory.
    \item ESC-like cells were identified as the population with the highest expression of Nanog and high expression of SSEA1 and Lin28.
        This was supported by high levels of H3K9ac, H4Kac, Cd54, pSrc and pERK.
    \item Lin28\hi{} cells were identified as named, supported by high expression of Cd24.
They were further distinguished from ESC-like cells by having higher expression of Cd140a and lower expression of Nanog.
    \item The Ki67\hi{} population was identified as named, supported by lower expression of Oct4.
    \item The mixed 4F population from the original study was highly heterogeneous and difficult to characterize.
We identified it as mostly Klf4\lo{}, containing subpopulations with intermediate c-Myc expression and both high and low Sox2 expression.
However, we were unable to identify the Oct4\lo{} subpopulations.
\end{itemize}
Many of these subpopulations were further resolved into distinct subsets based on specific markers such as IdU.
We also identified subpopulations that were not explicitly classified by Zunder \emph{et al.}
This includes a potentially apoptotic population with cleaved Caspase-3;
    a SC4-like subpopulation with phosphorylated STAT3, AMPK and PLK1;
    and a small set of MEF-like/mixed 4F cells that express Thy1, Sox2 and Oct4.
%   and an Oct\hi{}Klf4\hi{} population of cells expressing higher levels of Cyclin B1.


%%%% FIGURES BEGIN HERE %%%%
\newpage

\begin{figure}[p]
    \begin{center}
        \includegraphics[width=0.32\textwidth]{../real/neighbours/Cytobank_43324_4FI.pdf}
        \includegraphics[width=0.32\textwidth]{../real/neighbours/Cytobank_43324_NG.pdf}
        \includegraphics[width=0.32\textwidth]{../real/neighbours/Cytobank_43324_NN.pdf}          
    \end{center}
    \caption{
        Euclidean distance from each cell to its nearest neighbours as a function of the number of markers.
        For each cell in the first sample of each time course in the MEF reprogramming study, its nearest neighbours were identified in the full (36-dimensional) space.
        A subset of 9-25 markers were randomly chosen and the distance to each nearest neighbour was recalculated for each cell in the reduced space.
        The 10\textsuperscript{th} nearest neighbour was used for Oct4-GFP and Nanog-Neo, while the 1\textsuperscript{st} nearest neighbour was used for Nanog-GFP.
        Each point represents the \revised{mean} distance across all cells for a given number of markers, while the error bar represents the \revised{sample standard deviation across cells}.
        This was also repeated using all 36 markers.
        The red line marks the hypersphere radius that is used for each number of markers.
    }
    \label{fig:radius}
\end{figure}

\begin{figure}[p]
    \begin{center}
        \includegraphics[width=0.6\textwidth]{../real/neighbours/nvd_Cytobank_43324_4FI.pdf}
    \end{center}
    \caption{The \revised{dependence} of the hypersphere radius (based on the choice of $r_0\sqrt{2}$) on the cell count for each hypersphere in the Oct4-GFP time course.
        For each hypersphere \revised{centred on a cell}, the radius required to include a \revised{certain number of nearest neighbours} is computed.
        The distribution of radii across all hyperspheres is shown as a boxplot for each \revised{neighbour}.
        The red line represents the \revised{default} $r_0\sqrt{2}=0.5$.
        \revised{The blue lines mark the $r_0\sqrt{2}$ corresponding to median distances for including 8 and 30 neighbours.}
    }
    \label{fig:nvd}
\end{figure}

\begin{table}[btp]
\caption{Effect of changes to the hypersphere radius on the DA analysis.
The number of hyperspheres retained after filtering is shown, along with the number detected with significant differences at a spatial FDR of 5\%.
In each altered analysis, the number of DA hyperspheres gained or lost relative to the original analysis is reported.
The effect of altering the bandwidth (BW) for spatial FDR calculation was also tested, by defining the bandwidth using the 20\textsuperscript{th} (smaller) and 100\textsuperscript{th} nearest neighbour (larger).
}
\label{tab:param}
\begin{center}
\begin{tabular}{l l r r r r r}
\hline
\textbf{Dataset} & \textbf{Statistic} & \textbf{Original} & \multicolumn{2}{c}{\textbf{Value of $r_0\sqrt{2}$}} & \multicolumn{2}{c}{\textbf{Bandwidth}} \\
                 &                    &          & \makebox[0.4in][r]{\textit{0.48}} & \makebox[0.6in][r]{\textit{0.52}} 
                                                 & \makebox[0.6in][r]{\textit{Smaller}} & \makebox[0.6in][r]{\textit{Larger}} \\
\hline
Oct4-GFP & Total & 7720 & 4984 & 10799 & 7720 & 7720 \\
 & Significant & 7416 & 4837 & 10242 & 7418 & 7414 \\
 & Gained & - & 35 & 2916 & 2 & 0 \\
 & Lost & - & 2614 & 90 & 0 & 2 \\
\hline
Nanog-GFP & Total & 6297 & 4137 & 8700 & 6297 & 6297 \\
 & Significant & 5947 & 3917 & 8291 & 5947 & 5944 \\
 & Gained & - & 63 & 2389 & 0 & 0 \\
 & Lost & - & 2093 & 45 & 0 & 3 \\
\hline
Nanog-Neo & Total & 22043 & 15025 & 28944 & 22043 & 22043 \\
 & Significant & 21532 & 14663 & 28271 & 21532 & 21532 \\
 & Gained & - & 53 & 6809 & 0 & 0 \\
 & Lost & - & 6922 & 70 & 0 & 0 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{figure}[tbp]
    \begin{center}
        \includegraphics[width=0.8\textwidth,trim=10mm 50mm 10mm 50mm,clip]{../simulations/radius_setup.pdf}
    \end{center}
    \caption{\revised{Schematic of the simulation design for examining the effect of increasing the hypersphere radius.
        Here, the experimental design consists of two samples from different conditions and 30 markers.
        Two subpopulations are present in the high-dimensional space, one of which is DA between samples (red) and the other is not (grey).
        Cells from each subpopulation (10000 each) are uniformly distributed within a 30-dimensional sphere of radius (referred to as ``size'', above) of length $0.5\sqrt{30}$, centred at the subpopulation mean (crosses).
        The means of the two subpopulations are separated by a distance of 0.5 in each dimension.
        Each hypersphere's position is calculated by taking the median in each dimension across all cells in the hypersphere.
        Note that the radii of the hyperspheres and subpopulations need not be identical.
    }
    }
    \label{fig:radius_schematic}
\end{figure}

\begin{figure}[tbp]
    \begin{center}
        \includegraphics[width=\textwidth]{pics/Figure_RadiusSim.pdf}
    \end{center}
    \caption{\revised{Simulation results demonstrating the effect of expanding the radius on the median-based position of each hypersphere.
        For each hypersphere, the distance between its position and the mean of the DA subpopulation was computed and plotted against the log-fold change in abundance between samples.
        Hyperspheres were coloured based on whether they were centred on cells from the DA (red) or non-DA (grey) subpopulations.
        The vertical dashed line marks the distance corresponding to the size of the DA subpopulation, while the horizontal dashed line represents a $\log_2$-fold change of 1.
        This process was repeated to create plots for a range of increasing hypersphere radii and for different simulation scenarios.
        Scenario 1 refers to the default scenario described in Supplementary Figure~\ref{fig:radius_schematic}, while scenario 2 involves decreasing the number of cells in the DA subpopulation to 5000; scenario 3 increases the distance between subpopulations to 0.8 in all dimensions; and scenario 3 decreases the size of the DA subpopulation to $0.2\sqrt{30}$.
    }
    }
    \label{fig:radius_position}
\end{figure}

\begin{figure}[tbp]
    \begin{center}
        \includegraphics[width=0.32\textwidth,page=2,trim=0mm 0mm 0mm 20mm,clip]{../refdata/pics/ecdf_raw.pdf}
        \includegraphics[width=0.32\textwidth,page=2,trim=0mm 0mm 0mm 20mm,clip]{../refdata/pics/ecdf_rnorm.pdf}
        \includegraphics[width=0.32\textwidth,page=2,trim=0mm 0mm 0mm 20mm,clip]{../refdata/pics/ecdf_qnorm.pdf}
    \end{center}
    \caption{\revised{Empirical cumulative distribution functions for CD19 intensities of BMMCs from five healthy, untreated individuals in the Levine \textit{et al.} data set, before normalization (left), after range-based normalization (middle) and after quantile normalization (right).
        Each curve represents a separate individual and is labelled with a different colour.
        The same logicle transformation was applied to all intensities.
    }
    }
    \label{fig:quantile}
\end{figure}

\begin{figure}[tbp]
    \begin{center}
        \includegraphics[width=0.49\textwidth,page=1]{../simulations/plot_shift.pdf}
        \includegraphics[width=0.49\textwidth,page=2]{../simulations/plot_shift.pdf}
    \end{center}
    \caption{\revised{Observed type I error rates for the simulation of intensity shifts.
        Simulated data sets were constructed from each reprogramming time course by resampling cells.
        For each marker, a random intensity shift was sampled from a Normal distribution and added to the intensities for all cells in each sample.
        Values of 0 (no shift) to 0.3 (moderate shift) were used as the standard deviation of the shift distribution.
        In each simulation, the observed type I error rate was calculated at a nominal threshold of 0.01 (left).
        The common dispersion estimate across all hyperspheres was also computed (right).
        All values represent the mean across 50 simulation iterations, with error bars representing the standard error.
    }
}
\label{fig:shift_sim}
\end{figure}

\begin{figure}[tbp]
    \begin{center}
    \includegraphics[width=0.6\textwidth]{../real/analysis/pics/dispersions.pdf}
    \end{center}
    \caption{NB dispersion estimates for all hyperspheres in each time course of the MEF reprogramming data set, plotted against the average number of cells across all samples in each hypersphere.}
\end{figure}

\begin{figure}[tbp]
    \begin{center}
        \includegraphics[width=\textwidth]{pics/Figure_FDRpartitions.pdf}
    \end{center}
    \caption{\revised{Assignment of hyperspheres to hypothetical partitions.
        Each point represents the median-based position of a hypersphere in a data set with two markers, coloured based on its test outcome
        (red for significant increases in abundance between conditions, blue for significant decreases, white for no significant differences, and grey for false positives).
        Each hypersphere is assigned to a partition, and the test outcome of each partition is defined as that of its hyperspheres.
        For simplicity, all hyperspheres in a partition have the same outcome here.
        The aim is to control the spatial FDR across partitions, rather than the FDR across hyperspheres.
        In this example, the spatial FDR is 25\% while the FDR across hyperspheres is 5\%.
    }
}
    \label{fig:fdrdemo}
\end{figure}

%\begin{figure}[tbp]
%    \begin{center}
%        \includegraphics[width=0.6\textwidth]{../simulations/FDR_setup.pdf}
%    \end{center}
%    \caption{\revised{A PCA plot of the positions of the detected hyperspheres in the FDR control simulation based on the Oct4-GFP time course.
%        Detected hyperspheres were defined using the na\"ive BH method to nominally control the FDR at 5\%.
%        Each hypersphere was assigned to a pixel of width 0.5 based on its coordinates in the two-dimensional space defined by the first two principal components.
%        Red pixels contain a majority of hyperspheres that are truly differential, while grey pixels contain only non-DA hyperspheres (i.e., false positives).
%        For the latter, the depth of colour is proportional to the number of hyperspheres.
%    } 
%    }
%\end{figure}

\begin{figure}[tbp]
    \begin{center}
        \includegraphics[width=\textwidth,trim=25mm 25mm 0mm 40mm,clip]{../real/analysis/pics/shiny.png}
    \end{center}
    \caption{\revised{Screenshot of the user interface for detailed interpretation of non-redundant hyperspheres.
            Each density curve represents the distribution of intensities across all cells for a particular marker.
            The red dot indicates the median intensity for the hypersphere being examined, while the area under the curve is coloured depending on whether the median is high (yellow) or low (purple) compared to the intensities for the majority of cells.
            Each hypersphere can be annotated based on its intensities (e.g., to describe the subpopulation that it represents), and these labels are stored in memory for future use.
            In addition, the closest non-redundant hyperspheres that have already been labelled are shown for each new hypersphere, to simplify further labelling of nearby hyperspheres in the high-dimensional space.
        }
}
\label{fig:shiny}
\end{figure}

\clearpage
\newcommand{\bigfigopt}[1]{\includegraphics[width=\textwidth,draft]{#1}}

\begin{figure}[p]
    \begin{center}
    \bigfigopt{../real/analysis/pics/Cytobank_43324_4FI_markers.png}
    \end{center}
    \caption{
        Marker intensities of differentially abundant subpopulations in the Oct4-GFP time course, detected at a spatial FDR of 5\%.
        Each plot corresponds to a marker while each point represents a hypersphere, coloured according to its median intensity for the corresponding marker.
        The boundaries of the colour range for each marker were set to the 1\textsuperscript{st} and 99\textsuperscript{th} percentiles of the intensities for all cells.
    }
    \label{fig:oct4_markers}
\end{figure}

\begin{figure}[p]
    \begin{center}
    \includegraphics[width=0.6\textwidth]{../real/analysis/pics/Cytobank_43324_NG_logFC.png}
    \end{center}
    \caption{
        Differentially abundant subpopulations in the Nanog-GFP time course, detected at a spatial FDR of 5\%.
        Each point represents a hypersphere and is coloured according to its log-fold change in abundance over time.
        Grey points are hyperspheres with significant but non-linear changes in abundance.
    }
\end{figure}

\begin{figure}[p]
    \begin{center}
    \bigfigopt{../real/analysis/pics/Cytobank_43324_NG_markers.png}
    \end{center}
    \caption{
        Marker intensities of differentially abundant subpopulations in the Nanog-GFP time course, detected at a spatial FDR of 5\%.
        Each plot corresponds to a marker while each point represents a hypersphere, coloured according to its median intensity for the corresponding marker.
    }
\end{figure}

\begin{figure}[p]
    \begin{center}
    \includegraphics[width=0.6\textwidth]{../real/analysis/pics/Cytobank_43324_NN_logFC.png}
    \end{center}
    \caption{
        Differentially abundant subpopulations in the Nanog-Neo time course, detected at a spatial FDR of 5\% and coloured according to the log-fold change in abundance over time.
    }
\end{figure}

\begin{figure}[p]
    \begin{center}
    \bigfigopt{../real/analysis/pics/Cytobank_43324_NN_markers.png}
    \end{center}
    \caption{
        Marker intensities of differentially abundant subpopulations in the Nanog-Neo time course, detected at a spatial FDR of 5\%.
        Each plot corresponds to a marker while each point represents a hypersphere, coloured according to its median intensity for the corresponding marker.
    }
\end{figure}

\begin{figure}[p]
    \begin{center}
        \includegraphics[width=0.6\textwidth]{../real/analysis/pics/novel_timecourse.pdf}
    \end{center}
    \caption{
        Cell abundance for the novel SC4-like subpopulation with active STAT3, AMPK and PLK1 signalling, as a function of time in the Oct4-GFP reprogramming time course.
        Abundances are shown as a percentage of the total number of cells in each sample.
        Each line represents the abundance for each hypersphere representing the subpopulation, while the black line is the average across all hyperspheres.
    }
\end{figure}

\begin{figure}[p]
\begin{center}
\includegraphics[width=0.49\textwidth]{../simulations/cluster_setup.pdf}
\includegraphics[width=0.49\textwidth]{../simulations/plot_cluster.pdf}
\end{center}
\caption{Relative performance of cluster-based DA analyses in simulated data.
    Left: a PCA plot to illustrate the simulation design.
    A non-DA continuum of cells (grey) is present, in which DA subpopulations exclusive to the first (blue) or second conditions (red) are nested.
    Right: detection frequencies for each of the two DA subpopulations across 20 simulation iterations.
    In each iteration, DA analyses were performed using a cluster-based approach with varying numbers of clusters $k$ or with hyperspheres.
}
\label{fig:clustersim}
\end{figure}

\begin{figure}[p]
\begin{center}
\includegraphics[width=0.8\textwidth]{../real/clustering/Cytobank_43324_4FI_clusters.png}
\end{center}
\caption{DA clusters from a cluster-based analysis of the Oct4-GFP time course, mapped onto the $t$-SNE plot of DA hyperspheres.
Each coloured point represents the centre of a DA cluster that was detected at a FDR of 5\%.
Detected clusters are shown from several analyses performed with varying numbers of clusters $k$.
The colour of each point is based on the log-fold change per day in the cell abundance of the corresponding cluster.
DA hyperspheres are shown as a grey outline for comparison.
}
\end{figure}

\end{document}
